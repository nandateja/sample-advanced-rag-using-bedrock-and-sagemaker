{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fe8a71-6170-4caf-a3b9-e578c1c5f201",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context.\n",
    "\n",
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66330105-e1f4-46f3-9b36-9f7560407522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03249f43",
   "metadata": {},
   "source": [
    "## RAG with a simple question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12750c99",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199b3bd-3f66-4a29-9929-83cb9efa5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Base ID - Choose from different chunking strategies (Fixed, Hierarchical, or Semantic)\n",
    "kb_id = variables[\"kbSemanticChunk\"] \n",
    "\n",
    "# Bedrock Model ARN - Using Amazon Nova Lite for inference\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Number of relevant documents to retrieve for RAG\n",
    "number_of_results = 5\n",
    "\n",
    "# Configuration for text generation - Controls output length, randomness, and diversity\n",
    "generation_configuration = {\n",
    "    'inferenceConfig': {\n",
    "        'textInferenceConfig': {\n",
    "            'maxTokens': 4096,  # Maximum number of tokens in the generated response\n",
    "            'stopSequences': [],  # List of sequences that indicate stopping points\n",
    "            'temperature': 0.2,  # Controls randomness (lower values = more deterministic output)\n",
    "            'topP': 0.5  # Controls diversity of output by considering top P probability mass\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29db82",
   "metadata": {},
   "source": [
    "### Retrieve and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00f7db-57bc-4a91-b9dc-6ba3fef14917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query to search relevant knowledge base documents and generate an answer\n",
    "# query = \"What are three sub-tasks in question answering over knowledge bases?\"\n",
    "query = \"What were the third-person view games?\"\n",
    "\n",
    "# Perform retrieval-augmented generation (RAG) using the knowledge base\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": generation_configuration,  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)\n",
    "\n",
    "# Display the full response including citations for retrieved documents\n",
    "print('----------------- Citations ------------------')\n",
    "print(json.dumps(response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c92234",
   "metadata": {},
   "source": [
    "## Improve RAG quality with Enhanced Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab045f5",
   "metadata": {},
   "source": [
    "### Importance of Prompt Engineering\n",
    "Prompt engineering refers to the practice of optimizing textual input to a large language model (LLM) to improve output and receive the responses you want. Prompting helps an LLM perform a wide variety of tasks, including classification, question answering, code generation, creative writing, and more. The quality of prompts that you provide to a LLM can impact the quality of the model's responses. <br/>\n",
    " \n",
    "\n",
    "### Useful techniques to improve prompts for Amazon Nova models\n",
    "Please refer [link](https://docs.aws.amazon.com/nova/latest/userguide/prompting.html) for the best practice of prompt engineering with Amazon Nova models. Fllowings are a few highlights:\n",
    "* Create precise prompts. Provide contextual information, speficy the output format and style, and provide clear prompt sections.\n",
    "* Use system propmts to define how the model will repond.\n",
    "* Give Amazon Nova time to think. For example, add ```\"Think step-by-step.\"``` at the end of your query.\n",
    "* Provide examples.\n",
    "\n",
    "### Tips for using prompts in RAG\n",
    "* Provide Prompt Template: As with other functionalities, enhancing the system prompt can be beneficial. You can define the RAG Systems description in the system prompt, outlining the desired persona and behavior for the model.\n",
    "* Use Model Instructions: Additionally, you can include a dedicated ```\"Model Instructions:\"``` section within the system prompt, where you can provide specific guidelines for the model to follow. For instance, you can list instructions such as: ```In this example session, the model has access to search results and a user's question, its job is to answer the user's question using only information from the search results.```\n",
    "* Avoid Hallucination by restricting the instructions: Bring more focus to instructions by clearly mentioning \"DO NOT USE INFORMATION THAT IS NOT IN SEARCH RESULTS!\" as a model instruction so the answers are grounded in the provided context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A prompt template with Model Instructions:\n",
    "prompt_template = \"\"\"\n",
    "You are a game sales analyst. Based on the search results, answer questions from users.\n",
    "\n",
    "Model Instructions:\n",
    "- Provide a simple answer first, followed by bullets which support the answer. \n",
    "Bullets include citations from the search results.\n",
    "- When referring specific games, specify the year of publishment and the publisher.\n",
    "- In case the question requires multi-hop reasoning,\n",
    "you should find relevant information from search\n",
    "results and summarize the answer based on relevant\n",
    "information with logical reasoning.\n",
    "- If the search results do not contain information\n",
    "that can answer the question, please state that you\n",
    "could not find an exact answer to the question, and\n",
    "if search results are completely irrelevant, say\n",
    "that you could not find an exact answer, then summarize\n",
    "search results.\n",
    "- DO NOT USE INFORMATION THAT IS NOT IN SEARCH RESULTS!\n",
    "\n",
    "$Query$\n",
    "Resource: $search_results$\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676924ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complicated query\n",
    "\n",
    "query = \"How successful were third-person action games?\"\n",
    "\n",
    "# Perform RAG with/without the prompt template\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": {**generation_configuration\n",
    "                                        #, \"promptTemplate\":{\"textPromptTemplate\": prompt_template} # Comment in/out to test the effect of the Prompt Template\n",
    "                                    },  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
