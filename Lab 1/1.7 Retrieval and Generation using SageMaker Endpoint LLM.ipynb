{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61938163-1eb5-42b8-96d7-7937cb1dcfda",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with SageMaker Endpoint LLM\n",
    "\n",
    "## Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using a SageMaker-hosted large language model (LLM). We will retrieve relevant documents from a knowledge base and use the LLM to generate responses based on the retrieved information.  \n",
    "\n",
    "## Key Steps:  \n",
    "- Configure and query a knowledge base for relevant documents.  \n",
    "- Use a SageMaker-hosted LLM to generate contextual responses.  \n",
    "- Optimize retrieval and generation parameters for improved accuracy.  \n",
    "\n",
    "By the end of this notebook, you'll understand how to integrate SageMaker-hosted models into a RAG pipeline to enhance answer generation with domain-specific knowledge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d738b9a-e03a-48eb-a296-37dba1bfc52d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!pip install -Uq sagemaker boto3 langchain-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de4aef-bbd1-47f3-b4f3-6a69a238cfe0",
   "metadata": {},
   "source": [
    "Fetching existing resource information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb976f-39c7-442a-90a9-8261683f0016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86824309-c68a-4845-bcef-a75a91f3e1a5",
   "metadata": {},
   "source": [
    "In this example, you will use a model from [SageMaker Jumpstart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html). Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select FMs quickly based on pre-defined quality and responsibility metrics to perform tasks like article summarization and image generation.\n",
    "\n",
    "To load a model from SageMaker Jumpstart you need to specify a `model_id` and a `model_version`. The current list of models and versions can be found [here](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html).\n",
    "\n",
    "The Llama 3.2 3B model has a `model_id` of `meta-textgeneration-llama-3-2-3b-instruct`. To always use the latest version of the model, you can set `model_version` to `*`, but pining to a specific version is recommended to ensure consistency.\n",
    "\n",
    "Llama 3.2 3B was selected for this example because it is small, fast, and still supports a long context length (128k) to support larger retrievals if necessary for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4588ac-1fb5-4e88-aa00-17bf4fbe7f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LLM Configuration  \n",
    "model_id, model_version = \"meta-textgeneration-llama-3-8b-instruct\", \"2.11.2\"\n",
    "instance_type = \"ml.g5.4xlarge\"  # Define the SageMaker instance type for model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaef16a",
   "metadata": {},
   "source": [
    "> **Note**: The model deployment process for the SageMaker endpoint will take approximately 8-10 minutes to complete. During this time, the system is:\n",
    "> 1. Provisioning the required compute resources (GPU instances)\n",
    "> 2. Downloading and installing the model artifacts\n",
    "> 3. Configuring the inference environment\n",
    "> 4. Setting up auto-scaling and monitoring for the endpoint\n",
    ">\n",
    "> No further action is needed during this time. The cell will continue to execute until the endpoint is fully deployed and ready for inference. This is a one-time setup that will be used throughout the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b20928-cd3a-4243-9f0c-0fb164cd0d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "endpoint_name=f\"endpoint-llama-3-2-3b-instruct-{timestamp}\"\n",
    "\n",
    "# Load the JumpStart model with the specified model ID and instance type  \n",
    "llm_model = JumpStartModel(model_id=model_id, instance_type=instance_type)\n",
    "\n",
    "# Deploy the model as a SageMaker endpoint\n",
    "try:\n",
    "    llm_endpoint = llm_model.deploy(\n",
    "        accept_eula=True,  # Accept the model's End User License Agreement (EULA)  \n",
    "        initial_instance_count=1,  # Number of instances for hosting the model  \n",
    "        endpoint_name=endpoint_name  # Custom name for the deployed endpoint  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"New endpoint cannot be created. Please clean up a deployed endpoint from https://console.aws.amazon.com/sagemaker/home#/endpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c694a-e5e2-42f3-a58b-60eb4b94b5d9",
   "metadata": {},
   "source": [
    "#### Check the progress of a SageMaker Endpoint deployment [here](https://console.aws.amazon.com/sagemaker/home#/endpoints). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83fea6-8500-439e-8945-7e56b885cf21",
   "metadata": {},
   "source": [
    "Store the SageMaker endpoint name for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09242d1-4586-43e3-83ef-1243d4cd7327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the SageMaker endpoint name to the variables JSON file  \n",
    "with open(\"variables.json\", \"w\") as f:\n",
    "    json.dump({**variables, \"sagemakerLLMEndpoint\": llm_endpoint.endpoint_name}, f)\n",
    "\n",
    "# Print or return the deployed SageMaker endpoint name\n",
    "llm_endpoint.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd431c1e-4691-40b3-804e-10e09576fe19",
   "metadata": {},
   "source": [
    "# Retrieval and Generation using Bedrock Knowledge Bases and SageMaker hosted models\n",
    "\n",
    "With your endpoint successfully created, you can now use it as an output model in your RAG workflow. The following examples use the Amazon Bedrock Knowledge Bases that you created earlier for retrieval, combined with your SageMaker hosted model for generation. This hybrid approach results in a robust solution, combining the ease of use and managed aspects of Bedrock Knowledge Bases with the model flexibility and configuration controls of SageMaker hosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e138a0-75e5-4b02-ac48-e875a18ce5ca",
   "metadata": {},
   "source": [
    "## RAG Orchestration with LangChain\n",
    "\n",
    "To integrate LangChain with SageMaker endpoints, you first need to define a `ContentHandler`. Its purpose is to perform any transformations of the input/output data to match what the model expects and provide a processed output to client applications.\n",
    "\n",
    "This content handler specifies the input/output content types as UTF-8 encoded `application/json` and pulls the `generated_text` parameter from the json response as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874edcd-7384-4229-b69d-31c18ff7d8ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_aws.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "# Define a custom content handler for SageMaker LLM endpoint\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    # Specify content type for input and output\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    # Method to transform user input into the format expected by SageMaker\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})  # Format input as JSON\n",
    "        return input_str.encode(\"utf-8\")  # Encode to bytes\n",
    "\n",
    "    # Method to process the output from SageMaker\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))  # Decode response JSON\n",
    "        return response_json[\"generated_text\"]  # Extract the generated text from response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e6d41-68e5-47f5-9aa2-225e3ac12569",
   "metadata": {},
   "source": [
    "With your `ContentHandler` defined, the next step is to setup your retriever. This retriever is responsible for fetching the results from your Bedrock Knowledge Base so it can be provided as contextual input for generation.\n",
    "\n",
    "The `AmazonKnowledgeBasesRetriever` takes in a parameter of `knowledge_base_id` to select the appropriate knowledge base.  In this example the ids of `kbFixedChunk`, `kbHierarchicalChunk`, `kbSemanticChunk` refer to saved variables in your `variables.json` file that hold the actual knowledge base id.\n",
    "\n",
    "It also takes a `retrieval_config`, which at this time consists of a `vectorSearchConfiguration` with `numberOfResults` as the only configurable parameter. The `numberOfResults` parameter controls the maximum number of search results from the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c02366-2fda-45f8-899d-d068230be038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "# Knowledge Base Selection\n",
    "kb_id = variables[\"kbSemanticChunk\"]  # Options: \"kbFixedChunk\", \"kbHierarchicalChunk\", \"kbSemanticChunk\"\n",
    "\n",
    "# Retrieval-Augmented Generation (RAG) Configuration\n",
    "number_of_results = 3  # Number of relevant documents to retrieve\n",
    "\n",
    "# Initialize the retriever to fetch relevant documents from the Amazon Knowledge Base\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=kb_id,  # Specify the Knowledge Base ID to retrieve data from\n",
    "    region_name=variables[\"regionName\"],  # Define the AWS region where the Knowledge Base is located\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": number_of_results  # Set the number of relevant documents to retrieve\n",
    "        }\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abec14-766d-463f-9b01-cab8db265a7d",
   "metadata": {},
   "source": [
    "Next, define a prompt template for your call to the output model. \n",
    "\n",
    "Since you are using a Llama-3 model in this example, it needs to follow the [correct prompt format](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/).\n",
    "\n",
    "This template uses the following roles:\n",
    "- `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that help the model respond effectively.\n",
    "- `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n",
    "- `assistant`: Represents the response generated by the AI model based on the context provided in the system and user prompts.\n",
    "\n",
    "The fields `{context}` and `{question}` in the template will by dynamically injected as part of your RAG chain in a later step. These names are not hardcoded, but need to match what you specify when you build your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfe7e1-a41c-4718-8add-33f52bd81224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are an assistant for question-answering tasks. Answer the following question using the provided context. If you don't know the answer, just say \"I don't know.\".\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "<|start_header_id|>assistant<|end_header_id|> \n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2284175-f75e-47d7-a528-5e2172e7c2c7",
   "metadata": {},
   "source": [
    "Specify the parameters for generation.\n",
    "\n",
    "`temperature` – Affects the shape of the probability distribution for the predicted output and influences the likelihood of the model selecting lower-probability outputs.\n",
    "    - Choose a lower value to influence the model to select higher-probability outputs.\n",
    "    - Choose a higher value to influence the model to select lower-probability outputs.\n",
    "    - In technical terms, the temperature modulates the probability mass function for the next token. A lower temperature steepens the function and leads to more deterministic responses, and a higher temperature flattens the function and leads to more random responses.\n",
    "\n",
    "`top_k` – The number of most-likely candidates that the model considers for the next token.\n",
    "    - Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.\n",
    "    - Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs.\n",
    "    - For example, if you choose a value of 50 for Top K, the model selects from 50 of the most probable tokens that could be next in the sequence.\n",
    "\n",
    "`top_p` – The percentage of most-likely candidates that the model considers for the next token.\n",
    "    - Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.\n",
    "    - Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs.\n",
    "    - In technical terms, the model computes the cumulative probability distribution for the set of responses and considers only the top P% of the distribution. For example, if you choose a value of 0.8 for Top P, the model selects from the top 80% of the probability distribution of tokens that could be next in the sequence.\n",
    "\n",
    "`max_new_tokens` - The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "\n",
    "`stop` - Specify sequences of characters that stop the model from generating further tokens. If the model generates a stop sequence that you specify, it will stop generating after that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad50832-ce6c-4f26-8ede-11cd864185e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generation_configuration = {\n",
    "    \"temperature\": 0,  # Lower temperature for more deterministic responses  \n",
    "    \"top_k\": 10,  # Consider top 10 tokens at each generation step  \n",
    "    \"max_new_tokens\": 512,  # Maximum number of tokens to generate  \n",
    "    \"stop\": \"<|eot_id|>\"  # Stop sequence to end the response generation  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2493022-89ef-4e48-bf83-78cc7bce5e01",
   "metadata": {},
   "source": [
    "Here you will create your chain.\n",
    "\n",
    "1. Initialize the `ContentHandler` from above\n",
    "2. Create a `sagemaker-runtime` boto3 client for calling the endpoint\n",
    "3. Initialize the `PromptTemplate` from above\n",
    "4. Define a function to process the documents from the retriever. In this example, the document array is iterated through and the content is joined together using `\\n\\n` between them to break up the context.\n",
    "5. Finally, define your chain. Here, you'll define your chain using LangChain's [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/lcel/) to replace deprecated methods like [RetrievalQA](https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/). LCEL is designed to streamline the process of building useful apps with LLMs and combining related components.\n",
    "\n",
    "Your `qa_chain` will fill pass the `question` parameter from the invocation of the chain, and the context parameter by invoking the retriever and processing the result with the `format_docs` function. From there, those outputs are piped to the prompt template to fill in the defined placeholders, then sent to the `llm` SageMaker endpoint for generation. Finally, the model output is sent to the `StrOutputParser` to convert into a usable string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b423c31-f61c-462e-9302-de4af56ff729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "from langchain_aws.llms import SagemakerEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# Initialize content handler for processing model inputs/outputs\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "# Create a SageMaker runtime client to interact with the deployed model endpoint\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Initialize the LLM with the SageMaker endpoint\n",
    "llm = SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint.endpoint_name,  # Specify the SageMaker endpoint name\n",
    "        client=sagemaker_runtime,  # Attach the SageMaker runtime client\n",
    "        model_kwargs=generation_configuration,  # Pass the model configuration parameters\n",
    "        content_handler=content_handler,  # Use the custom content handler for formatting\n",
    "    )\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    results = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    return results\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e0412-6080-4dce-909d-a0845ffda874",
   "metadata": {},
   "source": [
    "You can now test your model with an example query. This query will get converted to an embedding and used for Knowledge Base search prior to question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bba00f-fad7-454e-bb23-4121e2fe3da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"In CONSOLIDATED STATEMENTS OF CASH FLOWS, How much did net income change in years 2022, 2023, 2024?\"\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {qa_chain.invoke(query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32f3c3-32c9-4c82-beab-a513ce362d3a",
   "metadata": {},
   "source": [
    "## RAG using boto3\n",
    "\n",
    "If you are not using LangChain, you can still perform the same tasks using the standard boto3 apis. This example shows how to use the Bedrock Knowledge Base `retrieve` API for retrieval, manually building the generation prompt, then using the SageMaker `invoke_endpoint` API to generate the output. This approach provides the most flexibility by leveraging low level constructs to build your own orchestration flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365830f-b266-4276-8e99-2e5b6566af6d",
   "metadata": {},
   "source": [
    "First, set up resources using configuration from above and define the boto3 client for Bedrock, you'll use this to perform retrievals from your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c97aa-8c66-4559-a65c-a4023eae6567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Bedrock client to interact with the Bedrock Knowledge Base\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Constants for Knowledge Base ID, SageMaker endpoint, and number of results to retrieve\n",
    "KNOWLEDGE_BASE_ID = kb_id\n",
    "ENDPOINT_NAME = llm_endpoint.endpoint_name\n",
    "NUM_RESULTS = number_of_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea1e16-2ac6-4213-8341-2cbf792c01bc",
   "metadata": {},
   "source": [
    "Next, you'll define a series of wrapper functions to simplify the steps of retrieval, prompt formatting, and generation.\n",
    "\n",
    "The `retrieve_from_bedrock` function takes an input query, Bedrock Knowledge Base id, the max number of results to retrieve from the knowledge base, and returns an array of text elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d93e7f-8383-4f25-8099-30edcd90efa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to retrieve relevant context from the Bedrock Knowledge Base\n",
    "def retrieve_from_bedrock(query, kb_id, num_results=5):\n",
    "    \"\"\"Retrieve relevant context from Bedrock Knowledge Base\"\"\"\n",
    "    try:\n",
    "        # Retrieve context based on the query using vector search configuration\n",
    "        response = bedrock_agent_runtime.retrieve(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            retrievalQuery={\n",
    "                'text': query  # The query text to search in the knowledge base\n",
    "            },\n",
    "            retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': num_results  # Adjust based on the number of results required\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        # Extract the 'text' from the retrieval results and return as a list\n",
    "        return [result['content']['text'] for result in response['retrievalResults']]\n",
    "    except Exception as e:\n",
    "        # Raise an error if the retrieval process fails\n",
    "        raise RuntimeError(f\"Bedrock retrieval failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee767ebe-4f2d-4c3f-86b8-5490dc3f1052",
   "metadata": {},
   "source": [
    "The `format_prompt` function takes in a user query and a context string from your knowledge base, then formats that into the desired prompt template for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02065c88-020a-431c-b400-cf12800a77f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to format the prompt for Llama 3 model using retrieved context\n",
    "def format_prompt(query, context):\n",
    "    \"\"\"Format prompt for Llama 3\"\"\"\n",
    "    # Format the complete prompt including system and user instructions\n",
    "    return f\"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        You are an assistant for question-answering tasks. Answer the following question using the provided context. If you don't know the answer, just say \"I don't know.\".\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Context: {context} \n",
    "        Question: {query}\n",
    "        <|start_header_id|>assistant<|end_header_id|> \n",
    "        Answer:\n",
    "        \"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b01ea-9bb7-4b6b-bef8-f06e6b724ffc",
   "metadata": {},
   "source": [
    "The `generate_response` function takes the fully formatted prompt and  SageMaker endpoint name, then uses it to invoke the endpoint to generate the RAG response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb035e66-cb68-408b-87cc-d74f6d9cbe0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to generate a response from the SageMaker endpoint based on the formatted prompt\n",
    "def generate_response(prompt, endpoint_name):\n",
    "    \"\"\"Generate response using SageMaker endpoint\"\"\"\n",
    "    # Initialize SageMaker runtime client\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    # Prepare the payload with prompt and generation parameters\n",
    "    payload = {\n",
    "        \"inputs\": prompt,  # The formatted prompt to pass to the model\n",
    "        \"parameters\": generation_configuration  # Additional parameters for the model (e.g., temperature, tokens)\n",
    "    }\n",
    "    try:\n",
    "        # Call the SageMaker endpoint to generate the response\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,  # SageMaker endpoint name\n",
    "            ContentType='application/json',  # Content type for the request\n",
    "            Body=json.dumps(payload)  # Send the payload as JSON\n",
    "        )\n",
    "\n",
    "        # Parse the response body\n",
    "        result = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "        \n",
    "        # Handle different response formats (list or dictionary)\n",
    "        if isinstance(result, list):\n",
    "            # If the result is a list, extract the generated text from the first element\n",
    "            return result[0]['generated_text']\n",
    "        elif 'generated_text' in result:\n",
    "            # If the result is a dictionary with 'generated_text', return the generated text\n",
    "            return result['generated_text']\n",
    "        elif 'generation' in result:\n",
    "            # Alternative format with 'generation' key\n",
    "            return result['generation']\n",
    "        else:\n",
    "            # Raise an error if the response format is unexpected\n",
    "            raise RuntimeError(\"Unexpected response format\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Raise an error if the generation process fails\n",
    "        raise RuntimeError(f\"Generation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdba3ad-a71b-4877-8202-278b418786b0",
   "metadata": {},
   "source": [
    "Finally, you can call the series of functions in order to invoke the workflow and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75167f7-fd5d-4429-9023-67493a7cfd0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant context from the Bedrock Knowledge Base based on the query\n",
    "context = retrieve_from_bedrock(query,KNOWLEDGE_BASE_ID,NUM_RESULTS)\n",
    "\n",
    "# Format the prompt by combining the user's query and the retrieved context\n",
    "prompt = format_prompt(query, context)\n",
    "\n",
    "# Generate the response using the formatted prompt by calling the SageMaker endpoint\n",
    "response = generate_response(prompt, ENDPOINT_NAME)\n",
    "\n",
    "# Print the user's query\n",
    "print(f\"Question: {query}\")\n",
    "\n",
    "# Uncomment below line if you want to debug and see the retrieved context\n",
    "# print(f\"Context: {context}\")\n",
    "\n",
    "# Print the generated answer from the model based on the query and context\n",
    "print(f\"Answer: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
