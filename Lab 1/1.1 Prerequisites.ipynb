{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e58641-fd5a-4b88-b07d-f187aba9676b",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. Prepare documents to be used in Amazon Bedrock Knowledge Base.\n",
    "2. Add metadata to the input documents for advanced query features (covered in Lab2).\n",
    "3. Create required AWS resources to run the Bedrock Knowledge Base service.\n",
    "4. Create an Amazon OpenSearch Service collection as a vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca54a5-0a31-493b-95d7-e14996bfa49b",
   "metadata": {},
   "source": [
    "### 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020dd9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc56f9-c071-4b62-820c-46aa8ecfd5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update python packages\n",
    "!pip install -U boto3 opensearch-py 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268baf2-ed49-4f96-bd3e-6f7c6f4fd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "# Initialize Boto3 session\n",
    "boto3_session = boto3.session.Session()\n",
    "credentials = boto3_session.get_credentials()\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "# Retrieve AWS account details\n",
    "sts_client = boto3_session.client(\"sts\")\n",
    "account_number = sts_client.get_caller_identity()[\"Account\"]\n",
    "role_arn = sts_client.get_caller_identity()[\"Arn\"]\n",
    "\n",
    "# Set up authentication for OpenSearch\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, \"aoss\")\n",
    "\n",
    "# Print account details for verification\n",
    "print(f\"AWS Account: {account_number}\")\n",
    "print(f\"Role ARN: {role_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15787e-c2c3-4d48-bc59-37aff4dcb017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resource names to be used in the workshop\n",
    "\n",
    "s3_bucket_name = f\"{account_number}-{region_name}-advanced-rag-workshop\"\n",
    "knowledge_base_name_aoss = \"advanced-rag-workshop-knowledgebase-aoss\"\n",
    "knowledge_base_name_graphrag = \"advanced-rag-workshop-knowledgebase-graphrag\"\n",
    "\n",
    "oss_vector_store_name = \"advancedrag\"\n",
    "oss_index_name = \"ws-index-\"\n",
    "\n",
    "# Print resource names for verification\n",
    "print(f\"S3 Bucket Name: {s3_bucket_name}\")\n",
    "print(f\"Knowledge Base (AOSS): {knowledge_base_name_aoss}\")\n",
    "print(f\"Knowledge Base (GraphRAG): {knowledge_base_name_graphrag}\")\n",
    "print(f\"OpenSearch Vector Store Name: {oss_vector_store_name}\")\n",
    "print(f\"OpenSearch Index Name Prefix: {oss_index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc22f1-4cd7-4ba3-b65c-2b4e4769c89f",
   "metadata": {},
   "source": [
    "### 2. Create required AWS resources "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11565dc-2a2c-43b4-8c88-1b5c20cc8f5a",
   "metadata": {},
   "source": [
    "#### IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e49512-7872-4610-a990-554b8fd5a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_excution_iam_role import AdvancedRagIamRoles\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize IAM role handler\n",
    "bedrock_execution_iam_role = AdvancedRagIamRoles(account_number, region_name)\n",
    "\n",
    "# Check if the role already exists\n",
    "iam_client = boto3.client(\"iam\", region_name=region_name)\n",
    "role_name = f\"advanced-rag-workshop-bedrock_execution_role-{region_name}\"\n",
    "bedrock_kb_execution_role_arn = \"\"\n",
    "\n",
    "try:\n",
    "    # Try to get the existing role\n",
    "    existing_role = iam_client.get_role(RoleName=role_name)\n",
    "    bedrock_kb_execution_role_arn = existing_role[\"Role\"][\"Arn\"]\n",
    "    print(f\"Policy and roles have been created already. ARN: {bedrock_kb_execution_role_arn}\")\n",
    "except Exception as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"NoSuchEntity\":\n",
    "        try:\n",
    "            # Role does not exist, create it\n",
    "            bedrock_kb_execution_role = bedrock_execution_iam_role.create_bedrock_execution_role(s3_bucket_name)\n",
    "            bedrock_kb_execution_role_arn = bedrock_kb_execution_role[\"Role\"][\"Arn\"]\n",
    "            print(f\"Created Bedrock Knowledge Base Execution Role ARN: {bedrock_kb_execution_role_arn}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Policies already exist. Please clean them up first.\")\n",
    "    else:\n",
    "        # Handle other client errors\n",
    "        print(\"Policy and roles have been created already.\")\n",
    "\n",
    "if not bedrock_kb_execution_role_arn:\n",
    "    print(\"WARNING: Could not determine the Bedrock KB execution role ARN.\")\n",
    "    bedrock_kb_execution_role_arn = f\"arn:aws:iam::{account_number}:role/{role_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f48397-af48-4cb0-81b0-7e4fd7f535aa",
   "metadata": {},
   "source": [
    "#### S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53de93-f6b9-4835-807c-eaf7dd772ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client with the specified AWS region\n",
    "s3 = boto3.client(\"s3\", region_name=region_name)\n",
    "\n",
    "try:\n",
    "    # Check if the S3 bucket already exists\n",
    "    s3.head_bucket(Bucket=s3_bucket_name)\n",
    "    print(f\"Bucket '{s3_bucket_name}' already exists.\")\n",
    "except:\n",
    "    # Create the S3 bucket if it does not exist\n",
    "    s3.create_bucket(Bucket=s3_bucket_name, CreateBucketConfiguration={'LocationConstraint': region_name})\n",
    "    print(f\"Bucket '{s3_bucket_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74286f9e-eee6-45c5-941c-ac4271ada540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to upload all files from a local directory to an S3 bucket\n",
    "def upload_directory(path, bucket_name, data_s3_prefix):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            key = f\"{data_s3_prefix}/{file}\"  # Construct the S3 object key\n",
    "            s3.upload_file(os.path.join(root, file), bucket_name, key)  # Upload the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624af69-c171-478f-a1be-3bce4c32aefb",
   "metadata": {},
   "source": [
    "### 3. Preparing Data Sources with .metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caef98c-b9c6-40a6-9dfa-c0ab2984f55c",
   "metadata": {},
   "source": [
    "### Role of Metadata While Indexing Data in Vector Databases  \n",
    "\n",
    "Metadata provides additional context and information about the documents, which can be used to filter, sort, and improve search accuracy. This not only helps reduce the search latency but also helps increase accuracy of responses.  \n",
    "\n",
    "The following are some key uses of metadata when loading documents into a vector data store:  \n",
    "\n",
    "- **Document Identification** – Metadata can include unique identifiers for each document, such as document IDs, URLs, or file names. These identifiers can be used to uniquely reference and retrieve specific documents from the vector data store.  \n",
    "- **Content Categorization** – Metadata can provide information about the content or category of a document, such as the subject matter, domain, or topic. This information can be used to organize and filter documents based on specific categories or domains.  \n",
    "- **Document Attributes** – Metadata can store additional attributes related to the document, such as the author, publication date, language, or any other relevant information. These attributes can be used for filtering, sorting, or faceted search within the vector data store.  \n",
    "- **Access Control** – Metadata can include information about access permissions or security levels associated with a document. This information can be used to control access to sensitive or restricted documents within the vector data store.  \n",
    "- **Relevance Scoring** – Metadata can be used to enhance the relevance scoring of search results. For example, if a user searches for documents within a specific date range or authored by a particular individual, the metadata can be used to prioritize and rank the most relevant documents.  \n",
    "- **Data Enrichment** – Metadata can be used to enrich the vector representations of documents by incorporating additional contextual information. This can potentially improve the accuracy and quality of search results.  \n",
    "- **Data Lineage and Auditing** – Metadata can provide information about the provenance and lineage of documents, such as the source system, data ingestion pipeline, or any transformations applied to the data. This information can be valuable for data governance, auditing, and compliance purposes.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0ca6f-68c8-40c9-bbef-75aa0fd4dc48",
   "metadata": {},
   "source": [
    "#### 3.1 Unstructured (PDF) document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd13c19-f890-4392-bf8d-fd7221f86346",
   "metadata": {},
   "source": [
    "#### Amazon Science papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933b1e9-74b4-4061-921a-7cc60c909d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define URLs of Amazon Science Publications to download as example documents\n",
    "urls = [\n",
    "    \"https://assets.amazon.science/44/ba/e16182124eac8687e89d3cb0ea3d/retrieval-reranking-and-multi-task-learning-for-knowledge-base-question-answering.pdf\",\n",
    "    \"https://assets.amazon.science/36/be/2669792342f2ba366ddca794069f/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\",\n",
    "    \"https://assets.amazon.science/a7/7c/8bdade5c4eda9168f3dee6434fff/pc-amazon-frontier-model-safety-framework-2-7-final-2-9.pdf\"\n",
    "]\n",
    "\n",
    "# Define standard filenames to maintain consistency when loading data to Amazon S3\n",
    "filenames = [\n",
    "    \"retrieval-reranking-and-multi-task-learning-for-knowledge-base-question-answering.pdf\",\n",
    "    \"practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\",\n",
    "    \"pc-amazon-frontier-model-safety-framework-2-7-final-2-9.pdf\"\n",
    "]\n",
    "\n",
    "# Create a local temporary directory to store downloaded files before uploading to S3\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Define local directory path for storing downloaded files\n",
    "local_data_path = \"./data/\"\n",
    "\n",
    "# Download files from URLs and save them in the local directory\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = os.path.join(local_data_path, filenames[idx])\n",
    "    urlretrieve(url, file_path)\n",
    "\n",
    "# Define metadata corresponding to each document for indexing in the vector database\n",
    "metadata = [\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Zhiguo Wang\", \"Patrick Ng\", \"Ramesh Nallapati\", \"Bing Xiang\"],\n",
    "            \"docType\": \"science\",\n",
    "            \"year\": 2021\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Marvin Dong\", \"Nischal Ashok Kumar\", \"Yiqun Hu\", \"Anuj Chauhan\", \"Chung-Wei Hang\", \"Shuaichen Chang\", \n",
    "                        \"Lin Pan\", \"Wuwei Lan\", \"Henry Zhu\", \"Jiarong Jiang\", \"Patrick Ng\", \"Zhiguo Wang\"],\n",
    "            \"docType\": \"science\",\n",
    "            \"year\": 2025\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Amazon\"],\n",
    "            \"docType\": \"science\",\n",
    "            \"year\": 2025\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save metadata as JSON files alongside the corresponding documents\n",
    "for i, file in enumerate(filenames):\n",
    "    with open(f\"{local_data_path}{file}.metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata[i], f)\n",
    "\n",
    "# Upload the directory to Amazon S3 under the 'pdf_documents' prefix\n",
    "upload_directory(local_data_path, s3_bucket_name, \"data/pdf_documents\")\n",
    "\n",
    "# Delete the local directory and its contents after upload to save space\n",
    "shutil.rmtree(local_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ce136-960b-4848-8a68-bbcf45aa6a19",
   "metadata": {},
   "source": [
    "#### Amazon 10-K filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be2c40-eabc-4661-bf0f-6800cd38433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define URLs of Amazon's 10-K reports to be downloaded as example documents\n",
    "urls = [\n",
    "    \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/e42c2068-bad5-4ab6-ae57-36ff8b2aeffd.pdf\",\n",
    "    \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf\",\n",
    "    \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf\"\n",
    "]\n",
    "\n",
    "# Define standard filenames to maintain consistency when loading data to Amazon S3\n",
    "filenames = [\n",
    "    \"Amazon-10k-2025.pdf\",\n",
    "    \"Amazon-10k-2024.pdf\",\n",
    "    \"Amazon-10k-2023.pdf\"\n",
    "]\n",
    "\n",
    "# Create a local temporary directory to store downloaded files before uploading to S3\n",
    "local_data_path = \"./data/\"\n",
    "os.makedirs(local_data_path, exist_ok=True)\n",
    "\n",
    "# Download files from URLs and save them in the local directory\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = os.path.join(local_data_path, filenames[idx])\n",
    "    urlretrieve(url, file_path)\n",
    "\n",
    "# Define metadata corresponding to each document for indexing in the vector database\n",
    "metadata = [\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Amazon\"],\n",
    "            \"docType\": \"10K Report\",\n",
    "            \"year\": 2025\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Amazon\"],\n",
    "            \"docType\": \"10K Report\",\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Amazon\"],\n",
    "            \"docType\": \"10K Report\",\n",
    "            \"year\": 2023\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save metadata as JSON files alongside the corresponding documents\n",
    "for i, file in enumerate(filenames):\n",
    "    metadata_file_path = os.path.join(local_data_path, f\"{file}.metadata.json\")\n",
    "    with open(metadata_file_path, \"w\") as f:\n",
    "        json.dump(metadata[i], f, indent=4)\n",
    "\n",
    "# Upload the directory to Amazon S3 under the 'pdf_documents' prefix\n",
    "upload_directory(local_data_path, s3_bucket_name, \"data/pdf_documents\")\n",
    "\n",
    "# Delete the local directory and its contents after upload to save space\n",
    "shutil.rmtree(local_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7d891-3087-47f0-a89f-eacca3e89a59",
   "metadata": {},
   "source": [
    "#### 3.2 Metadata customization for CSV files\n",
    "The data is downloaded from [here](https://github.com/ali-ce/datasets) and it is licensed under [Creative Commons Attribution-ShareAlike 4.0 International license](https://github.com/ali-ce/datasets/blob/master/README.md#:~:text=Creative%20Commons%20Attribution%2DShareAlike%204.0%20International%20License.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e32ceb-34e5-46de-9f5b-003f338825b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "# Define a function to generate JSON metadata from a CSV file\n",
    "def generate_json_metadata(csv_file, content_fields, metadata_fields, excluded_fields):\n",
    "    \"\"\"\n",
    "    Generates a JSON metadata file for a given CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (str): Path to the CSV file.\n",
    "        content_fields (list): List of fields that contain document content.\n",
    "        metadata_fields (list): List of fields to include as metadata.\n",
    "        excluded_fields (list): List of fields to exclude (automatically populated if empty).\n",
    "\n",
    "    The function reads the CSV file, extracts headers, and structures metadata accordingly.\n",
    "    It then saves the metadata as a JSON file in the same directory as the CSV file.\n",
    "    \"\"\"\n",
    "    # Open the CSV file and read its headers\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        headers = reader.fieldnames  # Get column names\n",
    "\n",
    "    # Define JSON structure for metadata\n",
    "    json_data = {\n",
    "        \"metadataAttributes\": {},\n",
    "        \"documentStructureConfiguration\": {\n",
    "            \"type\": \"RECORD_BASED_STRUCTURE_METADATA\",\n",
    "            \"recordBasedStructureMetadata\": {\n",
    "                \"contentFields\": [{\"fieldName\": field} for field in content_fields],\n",
    "                \"metadataFieldsSpecification\": {\n",
    "                    \"fieldsToInclude\": [{\"fieldName\": field} for field in metadata_fields],\n",
    "                    \"fieldsToExclude\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Determine fields to exclude (all fields not in content_fields or metadata_fields)\n",
    "    if not excluded_fields:\n",
    "        excluded_fields = set(headers) - set(content_fields + metadata_fields)\n",
    "\n",
    "    json_data[\"documentStructureConfiguration\"][\"recordBasedStructureMetadata\"][\"metadataFieldsSpecification\"][\"fieldsToExclude\"] = [\n",
    "        {\"fieldName\": field} for field in excluded_fields\n",
    "    ]\n",
    "\n",
    "    # Generate the output JSON file name\n",
    "    output_file = f\"{os.path.splitext(csv_file)[0]}.metadata.json\"\n",
    "\n",
    "    # Save metadata to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "    print(f\"JSON metadata file '{output_file}' has been generated.\")\n",
    "\n",
    "# Create a directory to store the video game CSV dataset\n",
    "local_dir = \"./videogame/\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Define the URL of the dataset and the local file path\n",
    "csv_url = \"https://raw.githubusercontent.com/ali-ce/datasets/master/Most-Expensive-Things/Videogames.csv\"\n",
    "csv_file_path = os.path.join(local_dir, \"video_games.csv\")\n",
    "\n",
    "# Download the CSV file\n",
    "response = requests.get(csv_url, verify=False)  # `verify=False` ignores SSL certificate issues\n",
    "if response.status_code == 200:\n",
    "    with open(csv_file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"CSV file downloaded successfully: {csv_file_path}\")\n",
    "else:\n",
    "    print(\"Failed to download the CSV file.\")\n",
    "\n",
    "# Generate JSON metadata for the downloaded CSV file\n",
    "generate_json_metadata(\n",
    "    csv_file=csv_file_path,\n",
    "    content_fields=[\"Description\"],\n",
    "    metadata_fields=[\"Year\", \"Developer\", \"Publisher\"],\n",
    "    excluded_fields=[]  # Automatically determine excluded fields\n",
    ")\n",
    "\n",
    "# Upload directory containing the CSV and metadata JSON to S3\n",
    "upload_directory(local_dir, s3_bucket_name, \"data/csv\")\n",
    "\n",
    "# Remove the local directory after upload to save space\n",
    "shutil.rmtree(local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39804c97-de8b-4ec5-99c5-e5afa076f649",
   "metadata": {},
   "source": [
    "### 4. Create a Vector Store using Amazon Open Search Serveless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7f5c7-71f3-4e80-9450-3c23be65cb8d",
   "metadata": {},
   "source": [
    "#### 4.1 Amazon OpenSearch Vector Collection  \n",
    "This will be used in Amazon Bedrock Knowledge Bases.  \n",
    "\n",
    "### **Code Steps:**  \n",
    "1. **Create security, network, and data access policies** within Amazon OpenSearch Serverless.  \n",
    "   - These will be assigned to the OpenSearch Vector Collection.  \n",
    "2. **Create an OpenSearch Serverless Vector Collection.**  \n",
    "3. **Retrieve the OpenSearch Serverless collection URL** for the Vector Collection created above.  \n",
    "4. **Wait for the Vector Collection** to reach the \"Ready\" state.  \n",
    "5. **Create an OpenSearch Serverless access policy** and attach it to the Bedrock execution role.\n",
    "\n",
    "\n",
    "> **Note**: This process will take approximately 4-5 minutes to complete. The system is creating security policies, network configurations, and a vector collection for storing embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f54cc5-d1e0-4ce6-be05-f066fcc90911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "# Initialize the OpenSearch Serverless client\n",
    "aoss = boto3.client(\"opensearchserverless\", region_name=region_name)\n",
    "\n",
    "print(\"Creating OpenSearch Serverless vector collection. This process will take approximately 4-5 minutes...\")\n",
    "\n",
    "# Create security, network, and data access policies within OpenSearch Serverless (OSS)\n",
    "# These policies are essential for the correct access configuration of the OSS\n",
    "try:\n",
    "    result = bedrock_execution_iam_role.create_policies_in_oss(\n",
    "        vector_store_name=oss_vector_store_name,\n",
    "        aoss_client=aoss,\n",
    "        bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn\n",
    "    )\n",
    "    if result is not None:  # Check if the result is valid\n",
    "        encryption_policy, network_policy, access_policy = result\n",
    "    else:\n",
    "        print(\"Policies already exist or were not created properly.\")\n",
    "        encryption_policy = network_policy = access_policy = None\n",
    "except Exception as e:\n",
    "    print(f\"Error creating policies: {str(e)}\")\n",
    "    encryption_policy = network_policy = access_policy = None\n",
    "\n",
    "# Check if the collection already exists before creation\n",
    "try:\n",
    "    response = aoss.batch_get_collection(names=[oss_vector_store_name])\n",
    "    if response['collectionDetails']:\n",
    "        print(f\"Collection '{oss_vector_store_name}' already exists.\")\n",
    "        # Extract the collection ID from the existing collection\n",
    "        collection_id = response['collectionDetails'][0]['id']\n",
    "        host = f\"{collection_id}.{region_name}.aoss.amazonaws.com\"  # Construct the host URL\n",
    "        print(f\"Collection Host URL: {host}\")\n",
    "    else:\n",
    "        # Create an OpenSearch Serverless Vector Collection\n",
    "        collection = aoss.create_collection(name=oss_vector_store_name, type='VECTORSEARCH')\n",
    "        collection_id = collection['createCollectionDetail']['id']\n",
    "        host = f\"{collection_id}.{region_name}.aoss.amazonaws.com\"  # Construct the host URL\n",
    "        print(f\"Collection Host URL: {host}\")\n",
    "except Exception:\n",
    "    print(f\"Collection '{oss_vector_store_name}' already exists or could not be created.\")\n",
    "\n",
    "# Wait for collection creation to complete\n",
    "# The creation process can take a few minutes, so we check the status periodically\n",
    "response = aoss.batch_get_collection(names=[oss_vector_store_name])\n",
    "print(response)\n",
    "# Periodically check the collection's status until it's no longer 'CREATING'\n",
    "while response['collectionDetails'][0]['status'] == 'CREATING':\n",
    "    print('Collection is still being created...')\n",
    "    time.sleep(10)  # Sleep for 10 seconds before checking again\n",
    "    response = aoss.batch_get_collection(names=[oss_vector_store_name])\n",
    "\n",
    "# Confirm successful collection creation\n",
    "print('\\nCollection successfully created!')\n",
    "\n",
    "# Create the OpenSearch Serverless access policy and attach it to the Bedrock execution role\n",
    "# This ensures that the execution role has the correct permissions to access the collection\n",
    "try:\n",
    "    bedrock_execution_iam_role.create_oss_policy_attach_bedrock_execution_role(\n",
    "        collection_id=collection_id,\n",
    "        bedrock_kb_execution_role=bedrock_kb_execution_role\n",
    "    )\n",
    "    # Wait for the data access rules to be enforced (may take a minute)\n",
    "    time.sleep(10)\n",
    "except Exception:\n",
    "    print(\"Policy already exists or has been attached previously.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d0dcd-8a40-4f1c-b8d7-633435a99573",
   "metadata": {},
   "source": [
    "#### 4.2 Create an index for the collection\n",
    "\n",
    "This index will be managed via Bedrock Knowledge Bases.\n",
    "\n",
    "**Code Steps:**\n",
    "\n",
    "1. **Create Index Body JSON**: Define the metadata or index structure that will be used for indexing in the OpenSearch Vector Collection.\n",
    "   \n",
    "2. **Create OpenSearch Object**: Instantiate an object of the `OpenSearch` class from the `opensearchpy` Python module. This object will be used to connect to the OpenSearch Vector Collection.\n",
    "\n",
    "3. **Create Index**: Using the OpenSearch object and the index body JSON, create the index in the OpenSearch Vector Collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013d7e6-faf3-42a7-92db-0cc1dabdc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "\n",
    "# Step 1: Set up AWS credentials for authentication with OpenSearch Service\n",
    "credentials = boto3.Session().get_credentials()  # Retrieves AWS credentials from the environment\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, \"aoss\")  # AWS authentication for OpenSearch\n",
    "\n",
    "# Define the base index name prefix\n",
    "oss_index_name = \"ws-index-\"\n",
    "\n",
    "# Step 2: Define the JSON body for index settings and mappings\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",  # Enable KNN (K-Nearest Neighbor) search\n",
    "       \"number_of_shards\": 1,  # Set the number of primary shards for the index\n",
    "       \"knn.algo_param.ef_search\": 512,  # KNN search efficiency parameter\n",
    "       \"number_of_replicas\": 0,  # Set the number of replicas to 0 (no redundancy)\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",  # Define a KNN vector field for storing embeddings\n",
    "            \"dimension\": 1024,  # Set the vector's dimension to 1024\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",  # Use the HNSW algorithm for KNN search\n",
    "                 \"engine\": \"faiss\",  # Use FAISS engine for efficient vector search\n",
    "                 \"space_type\": \"l2\"  # Use L2 (Euclidean) space for distance calculation\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"  # Define a text field for storing unstructured text\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"  # Define a text field for storing associated metadata\n",
    "        }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Step 3: Build the OpenSearch client using AWS credentials and settings\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],  # Provide OpenSearch host details\n",
    "    http_auth=awsauth,  # Use AWS authentication for API requests\n",
    "    use_ssl=True,  # Enable SSL for secure connection\n",
    "    verify_certs=True,  # Verify SSL certificates\n",
    "    connection_class=RequestsHttpConnection,  # Use RequestsHttpConnection for HTTP communication\n",
    "    timeout=300  # Set a timeout for the connection\n",
    ")\n",
    "\n",
    "# Step 4: Attempt to create multiple indices for different chunking strategies\n",
    "for strategy in [\"fixed\", \"hierarchical\", \"semantic\", \"custom\"]:\n",
    "    index_name = oss_index_name + strategy\n",
    "    try:\n",
    "        # Check if the index already exists\n",
    "        if oss_client.indices.exists(index=index_name):\n",
    "            print(f'Index \"{index_name}\" already exists. Skipping creation.')  # CHANGED\n",
    "            continue\n",
    "        \n",
    "        # Create the index if it doesn't exist\n",
    "        oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "        print(f'Creating Index: {index_name}...')  # Inform user about index creation\n",
    "    except RequestError as e:\n",
    "        print(f'Error while trying to create the index \"{index_name}\", with error {e.error}')  # CHANGED\n",
    "\n",
    "print('Index Creation Process Completed.')  # Inform user that the process is finished\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da1be5-ae6d-4fcf-afe5-26c7fe6e7389",
   "metadata": {},
   "source": [
    "### Export variables to a file for the next lab\n",
    "\n",
    "> **Note**: We're saving all the important configuration variables to a JSON file so they can be easily accessed in subsequent notebooks. This ensures consistency and prevents the need to recreate these resources for each notebook in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9231e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"w\") as f:\n",
    "    # Create a collection ARN using the standard format if needed\n",
    "    collection_arn = f\"arn:aws:aoss:{region_name}:{account_number}:collection/{collection_id}\"\n",
    "    \n",
    "    json.dump(\n",
    "        {\n",
    "            \"accountNumber\": account_number,\n",
    "            \"regionName\": region_name,\n",
    "            \"collectionArn\": collection_arn,\n",
    "            \"collectionId\": collection_id,\n",
    "            \"vectorIndexName\": oss_index_name,\n",
    "            \"bedrockExecutionRoleArn\": bedrock_kb_execution_role_arn,\n",
    "            \"s3Bucket\": s3_bucket_name\n",
    "        }, f, indent=4\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
