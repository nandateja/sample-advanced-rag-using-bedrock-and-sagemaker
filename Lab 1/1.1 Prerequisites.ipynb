{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e58641-fd5a-4b88-b07d-f187aba9676b",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. Prepare documents to be used in Amazon Bedrock Knowledge Base.\n",
    "2. Add metadata to the input documents for advanced query features (covered in Lab2).\n",
    "3. Create required AWS resources to run the Bedrock Knowledge Base service.\n",
    "4. Create an Amazon OpenSearch Service collection as a vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca54a5-0a31-493b-95d7-e14996bfa49b",
   "metadata": {},
   "source": [
    "### 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc56f9-c071-4b62-820c-46aa8ecfd5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update python packages\n",
    "!pip install -U boto3 opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268baf2-ed49-4f96-bd3e-6f7c6f4fd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "# Initialize Boto3 session\n",
    "boto3_session = boto3.session.Session()\n",
    "credentials = boto3_session.get_credentials()\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "# Retrieve AWS account details\n",
    "sts_client = boto3_session.client(\"sts\")\n",
    "account_number = sts_client.get_caller_identity()[\"Account\"]\n",
    "role_arn = sts_client.get_caller_identity()[\"Arn\"]\n",
    "\n",
    "# Set up authentication for OpenSearch\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, \"aoss\")\n",
    "\n",
    "# Print account details for verification\n",
    "print(f\"AWS Account: {account_number}\")\n",
    "print(f\"Role ARN: {role_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15787e-c2c3-4d48-bc59-37aff4dcb017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource names to be used in the workshop\n",
    "\n",
    "s3_bucket_name = f\"{account_number}-{region_name}-advanced-rag-workshop\"\n",
    "knowledge_base_name_aoss = \"advanced-rag-workshop-knowledgebase-aoss\"\n",
    "knowledge_base_name_graphrag = \"advanced-rag-workshop-knowledgebase-graphrag\"\n",
    "\n",
    "oss_vector_store_name = \"advancedrag\"\n",
    "oss_index_name = \"ws-index-\"\n",
    "\n",
    "# Print resource names for verification\n",
    "print(f\"S3 Bucket Name: {s3_bucket_name}\")\n",
    "print(f\"Knowledge Base (AOSS): {knowledge_base_name_aoss}\")\n",
    "print(f\"Knowledge Base (GraphRAG): {knowledge_base_name_graphrag}\")\n",
    "print(f\"OpenSearch Vector Store Name: {oss_vector_store_name}\")\n",
    "print(f\"OpenSearch Index Name Prefix: {oss_index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc22f1-4cd7-4ba3-b65c-2b4e4769c89f",
   "metadata": {},
   "source": [
    "### 2. Create required AWS resources "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11565dc-2a2c-43b4-8c88-1b5c20cc8f5a",
   "metadata": {},
   "source": [
    "#### IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e49512-7872-4610-a990-554b8fd5a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_excution_iam_role import AdvancedRagIamRoles\n",
    "\n",
    "# Initialize IAM role handler\n",
    "bedrock_execution_iam_role = AdvancedRagIamRoles(account_number, region_name)\n",
    "\n",
    "# Create Bedrock execution role\n",
    "bedrock_kb_execution_role = bedrock_execution_iam_role.create_bedrock_execution_role(s3_bucket_name)\n",
    "\n",
    "# Extract the ARN of the created role\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role[\"Role\"][\"Arn\"]\n",
    "\n",
    "# Print for verification\n",
    "print(f\"Bedrock Knowledge Base Execution Role ARN: {bedrock_kb_execution_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f48397-af48-4cb0-81b0-7e4fd7f535aa",
   "metadata": {},
   "source": [
    "#### S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53de93-f6b9-4835-807c-eaf7dd772ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client with the specified AWS region\n",
    "s3 = boto3.client(\"s3\", region_name=region_name)\n",
    "\n",
    "try:\n",
    "    # Check if the S3 bucket already exists\n",
    "    s3.head_bucket(Bucket=s3_bucket_name)\n",
    "    print(f\"Bucket '{s3_bucket_name}' already exists.\")\n",
    "except:\n",
    "    # Create the S3 bucket if it does not exist\n",
    "    s3.create_bucket(Bucket=s3_bucket_name, CreateBucketConfiguration={'LocationConstraint': region_name})\n",
    "    print(f\"Bucket '{s3_bucket_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74286f9e-eee6-45c5-941c-ac4271ada540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to upload all files from a local directory to an S3 bucket\n",
    "def upload_directory(path, bucket_name, data_s3_prefix):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            key = f\"{data_s3_prefix}/{file}\"  # Construct the S3 object key\n",
    "            s3.upload_file(os.path.join(root, file), bucket_name, key)  # Upload the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624af69-c171-478f-a1be-3bce4c32aefb",
   "metadata": {},
   "source": [
    "### 3. Preparing Data Sources with .metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caef98c-b9c6-40a6-9dfa-c0ab2984f55c",
   "metadata": {},
   "source": [
    "### Role of Metadata While Indexing Data in Vector Databases  \n",
    "\n",
    "Metadata plays a crucial role when loading documents into a vector data store in Amazon Bedrock. It provides additional context and information about the documents, which can be used for various purposes, such as filtering, sorting, and enhancing search capabilities.  \n",
    "\n",
    "The following are some key uses of metadata when loading documents into a vector data store:  \n",
    "\n",
    "- **Document Identification** – Metadata can include unique identifiers for each document, such as document IDs, URLs, or file names. These identifiers can be used to uniquely reference and retrieve specific documents from the vector data store.  \n",
    "- **Content Categorization** – Metadata can provide information about the content or category of a document, such as the subject matter, domain, or topic. This information can be used to organize and filter documents based on specific categories or domains.  \n",
    "- **Document Attributes** – Metadata can store additional attributes related to the document, such as the author, publication date, language, or any other relevant information. These attributes can be used for filtering, sorting, or faceted search within the vector data store.  \n",
    "- **Access Control** – Metadata can include information about access permissions or security levels associated with a document. This information can be used to control access to sensitive or restricted documents within the vector data store.  \n",
    "- **Relevance Scoring** – Metadata can be used to enhance the relevance scoring of search results. For example, if a user searches for documents within a specific date range or authored by a particular individual, the metadata can be used to prioritize and rank the most relevant documents.  \n",
    "- **Data Enrichment** – Metadata can be used to enrich the vector representations of documents by incorporating additional contextual information. This can potentially improve the accuracy and quality of search results.  \n",
    "- **Data Lineage and Auditing** – Metadata can provide information about the provenance and lineage of documents, such as the source system, data ingestion pipeline, or any transformations applied to the data. This information can be valuable for data governance, auditing, and compliance purposes.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0ca6f-68c8-40c9-bbef-75aa0fd4dc48",
   "metadata": {},
   "source": [
    "#### 3.1 Unstructured (PDF) document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd13c19-f890-4392-bf8d-fd7221f86346",
   "metadata": {},
   "source": [
    "#### Amazon Science papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933b1e9-74b4-4061-921a-7cc60c909d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define URLs of Amazon Science Publications to download as example documents\n",
    "urls = [\n",
    "    \"https://assets.amazon.science/44/ba/e16182124eac8687e89d3cb0ea3d/retrieval-reranking-and-multi-task-learning-for-knowledge-base-question-answering.pdf\",\n",
    "    \"https://assets.amazon.science/36/be/2669792342f2ba366ddca794069f/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\",\n",
    "    \"https://assets.amazon.science/a7/7c/8bdade5c4eda9168f3dee6434fff/pc-amazon-frontier-model-safety-framework-2-7-final-2-9.pdf\"\n",
    "]\n",
    "\n",
    "# Define standard filenames to maintain consistency when loading data to Amazon S3\n",
    "filenames = [\n",
    "    \"retrieval-reranking-and-multi-task-learning-for-knowledge-base-question-answering.pdf\",\n",
    "    \"practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\",\n",
    "    \"pc-amazon-frontier-model-safety-framework-2-7-final-2-9.pdf\"\n",
    "]\n",
    "\n",
    "# Create a local temporary directory to store downloaded files before uploading to S3\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Define local directory path for storing downloaded files\n",
    "local_data_path = \"./data/\"\n",
    "\n",
    "# Download files from URLs and save them in the local directory\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = os.path.join(local_data_path, filenames[idx])\n",
    "    urlretrieve(url, file_path)\n",
    "\n",
    "# Define metadata corresponding to each document for indexing in the vector database\n",
    "metadata = [\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Zhiguo Wang\", \"Patrick Ng\", \"Ramesh Nallapati\", \"Bing Xiang\"],\n",
    "            \"docType\": \"science\",\n",
    "            \"year\": 2021\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Marvin Dong\", \"Nischal Ashok Kumar\", \"Yiqun Hu\", \"Anuj Chauhan\", \"Chung-Wei Hang\", \"Shuaichen Chang\", \n",
    "                        \"Lin Pan\", \"Wuwei Lan\", \"Henry Zhu\", \"Jiarong Jiang\", \"Patrick Ng\", \"Zhiguo Wang\"],\n",
    "            \"docType\": \"science\",\n",
    "            \"year\": 2025\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Amazon\"],\n",
    "            \"docType\": \"science\",\n",
    "            \"year\": 2025\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save metadata as JSON files alongside the corresponding documents\n",
    "for i, file in enumerate(filenames):\n",
    "    with open(f\"{local_data_path}{file}.metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata[i], f)\n",
    "\n",
    "# Upload the directory to Amazon S3 under the 'pdf_documents' prefix\n",
    "upload_directory(local_data_path, s3_bucket_name, \"pdf_documents\")\n",
    "\n",
    "# Delete the local directory and its contents after upload to save space\n",
    "shutil.rmtree(local_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ce136-960b-4848-8a68-bbcf45aa6a19",
   "metadata": {},
   "source": [
    "#### Amazon 10-K filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be2c40-eabc-4661-bf0f-6800cd38433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define URLs of Amazon's 10-K reports to be downloaded as example documents\n",
    "urls = [\n",
    "    \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/e42c2068-bad5-4ab6-ae57-36ff8b2aeffd.pdf\",\n",
    "    \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf\",\n",
    "    \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf\"\n",
    "]\n",
    "\n",
    "# Define standard filenames to maintain consistency when loading data to Amazon S3\n",
    "filenames = [\n",
    "    \"Amazon-10k-2025.pdf\",\n",
    "    \"Amazon-10k-2024.pdf\",\n",
    "    \"Amazon-10k-2023.pdf\"\n",
    "]\n",
    "\n",
    "# Create a local temporary directory to store downloaded files before uploading to S3\n",
    "local_data_path = \"./data/\"\n",
    "os.makedirs(local_data_path, exist_ok=True)\n",
    "\n",
    "# Download files from URLs and save them in the local directory\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = os.path.join(local_data_path, filenames[idx])\n",
    "    urlretrieve(url, file_path)\n",
    "\n",
    "# Define metadata corresponding to each document for indexing in the vector database\n",
    "metadata = [\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Amazon\"],\n",
    "            \"docType\": \"10K Report\",\n",
    "            \"year\": 2025\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Amazon\"],\n",
    "            \"docType\": \"10K Report\",\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company\": \"Amazon\",\n",
    "            \"authors\": [\"Amazon\"],\n",
    "            \"docType\": \"10K Report\",\n",
    "            \"year\": 2023\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save metadata as JSON files alongside the corresponding documents\n",
    "for i, file in enumerate(filenames):\n",
    "    metadata_file_path = os.path.join(local_data_path, f\"{file}.metadata.json\")\n",
    "    with open(metadata_file_path, \"w\") as f:\n",
    "        json.dump(metadata[i], f, indent=4)\n",
    "\n",
    "# Upload the directory to Amazon S3 under the 'pdf_documents' prefix\n",
    "upload_directory(local_data_path, s3_bucket_name, \"pdf_documents\")\n",
    "\n",
    "# Delete the local directory and its contents after upload to save space\n",
    "shutil.rmtree(local_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7d891-3087-47f0-a89f-eacca3e89a59",
   "metadata": {},
   "source": [
    "#### 3.2 Metadata customization for CSV files\n",
    "The data is downloaded from [here](https://github.com/ali-ce/datasets) and it is licensed under [Creative Commons Attribution-ShareAlike 4.0 International license](https://github.com/ali-ce/datasets/blob/master/README.md#:~:text=Creative%20Commons%20Attribution%2DShareAlike%204.0%20International%20License.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e32ceb-34e5-46de-9f5b-003f338825b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "# Define a function to generate JSON metadata from a CSV file\n",
    "def generate_json_metadata(csv_file, content_fields, metadata_fields, excluded_fields):\n",
    "    \"\"\"\n",
    "    Generates a JSON metadata file for a given CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (str): Path to the CSV file.\n",
    "        content_fields (list): List of fields that contain document content.\n",
    "        metadata_fields (list): List of fields to include as metadata.\n",
    "        excluded_fields (list): List of fields to exclude (automatically populated if empty).\n",
    "\n",
    "    The function reads the CSV file, extracts headers, and structures metadata accordingly.\n",
    "    It then saves the metadata as a JSON file in the same directory as the CSV file.\n",
    "    \"\"\"\n",
    "    # Open the CSV file and read its headers\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        headers = reader.fieldnames  # Get column names\n",
    "\n",
    "    # Define JSON structure for metadata\n",
    "    json_data = {\n",
    "        \"metadataAttributes\": {},\n",
    "        \"documentStructureConfiguration\": {\n",
    "            \"type\": \"RECORD_BASED_STRUCTURE_METADATA\",\n",
    "            \"recordBasedStructureMetadata\": {\n",
    "                \"contentFields\": [{\"fieldName\": field} for field in content_fields],\n",
    "                \"metadataFieldsSpecification\": {\n",
    "                    \"fieldsToInclude\": [{\"fieldName\": field} for field in metadata_fields],\n",
    "                    \"fieldsToExclude\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Determine fields to exclude (all fields not in content_fields or metadata_fields)\n",
    "    if not excluded_fields:\n",
    "        excluded_fields = set(headers) - set(content_fields + metadata_fields)\n",
    "\n",
    "    json_data[\"documentStructureConfiguration\"][\"recordBasedStructureMetadata\"][\"metadataFieldsSpecification\"][\"fieldsToExclude\"] = [\n",
    "        {\"fieldName\": field} for field in excluded_fields\n",
    "    ]\n",
    "\n",
    "    # Generate the output JSON file name\n",
    "    output_file = f\"{os.path.splitext(csv_file)[0]}.metadata.json\"\n",
    "\n",
    "    # Save metadata to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "    print(f\"JSON metadata file '{output_file}' has been generated.\")\n",
    "\n",
    "# Create a directory to store the video game CSV dataset\n",
    "local_dir = \"./videogame/\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Define the URL of the dataset and the local file path\n",
    "csv_url = \"https://raw.githubusercontent.com/ali-ce/datasets/master/Most-Expensive-Things/Videogames.csv\"\n",
    "csv_file_path = os.path.join(local_dir, \"video_games.csv\")\n",
    "\n",
    "# Download the CSV file\n",
    "response = requests.get(csv_url, verify=False)  # `verify=False` ignores SSL certificate issues\n",
    "if response.status_code == 200:\n",
    "    with open(csv_file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"CSV file downloaded successfully: {csv_file_path}\")\n",
    "else:\n",
    "    print(\"Failed to download the CSV file.\")\n",
    "\n",
    "# Generate JSON metadata for the downloaded CSV file\n",
    "generate_json_metadata(\n",
    "    csv_file=csv_file_path,\n",
    "    content_fields=[\"Description\"],\n",
    "    metadata_fields=[\"Year\", \"Developer\", \"Publisher\"],\n",
    "    excluded_fields=[]  # Automatically determine excluded fields\n",
    ")\n",
    "\n",
    "# Upload directory containing the CSV and metadata JSON to S3\n",
    "upload_directory(local_dir, s3_bucket_name, \"csv\")\n",
    "\n",
    "# Remove the local directory after upload to save space\n",
    "shutil.rmtree(local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39804c97-de8b-4ec5-99c5-e5afa076f649",
   "metadata": {},
   "source": [
    "### 4. Create a Vector Store using Amazon Open Search Serveless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7f5c7-71f3-4e80-9450-3c23be65cb8d",
   "metadata": {},
   "source": [
    "#### 4.1 Amazon OpenSearch Vector Collection  \n",
    "This will be used in Amazon Bedrock Knowledge Bases.  \n",
    "\n",
    "### **Code Steps:**  \n",
    "1. **Create security, network, and data access policies** within Amazon OpenSearch Serverless.  \n",
    "   - These will be assigned to the OpenSearch Vector Collection.  \n",
    "2. **Create an OpenSearch Serverless Vector Collection.**  \n",
    "3. **Retrieve the OpenSearch Serverless collection URL** for the Vector Collection created above.  \n",
    "4. **Wait for the Vector Collection** to reach the \"Ready\" state.  \n",
    "5. **Create an OpenSearch Serverless access policy** and attach it to the Bedrock execution role.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f54cc5-d1e0-4ce6-be05-f066fcc90911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "# Initialize the OpenSearch Serverless client\n",
    "aoss = boto3.client(\"opensearchserverless\", region_name=region_name)\n",
    "\n",
    "# Create security, network, and data access policies within OpenSearch Serverless (OSS)\n",
    "# These policies are essential for the correct access configuration of the OSS\n",
    "encryption_policy, network_policy, access_policy = bedrock_execution_iam_role.create_policies_in_oss(\n",
    "    vector_store_name=oss_vector_store_name,\n",
    "    aoss_client=aoss,\n",
    "    bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn\n",
    ")\n",
    "\n",
    "# Create an OpenSearch Serverless Vector Collection\n",
    "# This collection will store vector data for efficient search and retrieval\n",
    "collection = aoss.create_collection(name=oss_vector_store_name, type='VECTORSEARCH')\n",
    "\n",
    "# Get the OpenSearch Serverless collection URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = f\"{collection_id}.{region_name}.aoss.amazonaws.com\"  # Construct the host URL\n",
    "print(f\"Collection Host URL: {host}\")\n",
    "\n",
    "# Wait for collection creation to complete\n",
    "# The creation process can take a few minutes, so we check the status periodically\n",
    "response = aoss.batch_get_collection(names=[oss_vector_store_name])\n",
    "\n",
    "# Periodically check the collection's status until it's no longer 'CREATING'\n",
    "while response['collectionDetails'][0]['status'] == 'CREATING':\n",
    "    print('Collection is still being created...')\n",
    "    time.sleep(30)  # Sleep for 30 seconds before checking again\n",
    "    response = aoss.batch_get_collection(names=[oss_vector_store_name])\n",
    "\n",
    "# Confirm successful collection creation\n",
    "print('\\nCollection successfully created!')\n",
    "\n",
    "# Create the OpenSearch Serverless access policy and attach it to the Bedrock execution role\n",
    "# This ensures that the execution role has the correct permissions to access the collection\n",
    "try:\n",
    "    bedrock_execution_iam_role.create_oss_policy_attach_bedrock_execution_role(\n",
    "        collection_id=collection_id,\n",
    "        bedrock_kb_execution_role=bedrock_kb_execution_role\n",
    "    )\n",
    "    # Wait for the data access rules to be enforced (may take a minute)\n",
    "    time.sleep(20)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"Error: Policy already exists or other issue encountered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d0dcd-8a40-4f1c-b8d7-633435a99573",
   "metadata": {},
   "source": [
    "#### 4.2 Create an index for the collection\n",
    "\n",
    "This index will be managed via Bedrock Knowledge Bases.\n",
    "\n",
    "**Code Steps:**\n",
    "\n",
    "1. **Create Index Body JSON**: Define the metadata or index structure that will be used for indexing in the OpenSearch Vector Collection.\n",
    "   \n",
    "2. **Create OpenSearch Object**: Instantiate an object of the `OpenSearch` class from the `opensearchpy` Python module. This object will be used to connect to the OpenSearch Vector Collection.\n",
    "\n",
    "3. **Create Index**: Using the OpenSearch object and the index body JSON, create the index in the OpenSearch Vector Collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013d7e6-faf3-42a7-92db-0cc1dabdc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "\n",
    "# Step 1: Set up AWS credentials for authentication with OpenSearch Service\n",
    "credentials = boto3.Session().get_credentials()  # Retrieves AWS credentials from the environment\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, \"aoss\")  # AWS authentication for OpenSearch\n",
    "\n",
    "# Define the base index name prefix\n",
    "oss_index_name = \"ws-index-\"\n",
    "\n",
    "# Step 2: Define the JSON body for index settings and mappings\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",  # Enable KNN (K-Nearest Neighbor) search\n",
    "       \"number_of_shards\": 1,  # Set the number of primary shards for the index\n",
    "       \"knn.algo_param.ef_search\": 512,  # KNN search efficiency parameter\n",
    "       \"number_of_replicas\": 0,  # Set the number of replicas to 0 (no redundancy)\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",  # Define a KNN vector field for storing embeddings\n",
    "            \"dimension\": 1024,  # Set the vector's dimension to 1024\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",  # Use the HNSW algorithm for KNN search\n",
    "                 \"engine\": \"faiss\",  # Use FAISS engine for efficient vector search\n",
    "                 \"space_type\": \"l2\"  # Use L2 (Euclidean) space for distance calculation\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"  # Define a text field for storing unstructured text\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"  # Define a text field for storing associated metadata\n",
    "        }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Step 3: Build the OpenSearch client using AWS credentials and settings\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],  # Provide OpenSearch host details\n",
    "    http_auth=awsauth,  # Use AWS authentication for API requests\n",
    "    use_ssl=True,  # Enable SSL for secure connection\n",
    "    verify_certs=True,  # Verify SSL certificates\n",
    "    connection_class=RequestsHttpConnection,  # Use RequestsHttpConnection for HTTP communication\n",
    "    timeout=300  # Set a timeout for the connection\n",
    ")\n",
    "\n",
    "# Step 4: Attempt to create multiple indices for different chunking strategies\n",
    "try:\n",
    "    # Create four different indices with the same settings and mappings\n",
    "    oss_client.indices.create(index=oss_index_name+\"fixed\", body=json.dumps(body_json))\n",
    "    oss_client.indices.create(index=oss_index_name+\"hierarchical\", body=json.dumps(body_json))\n",
    "    oss_client.indices.create(index=oss_index_name+\"semantic\", body=json.dumps(body_json))\n",
    "    oss_client.indices.create(index=oss_index_name+\"custom\", body=json.dumps(body_json))\n",
    "\n",
    "    print('Creating Indices...')  # Inform user that indices are being created\n",
    "    # Index creation can take up to a minute, so sleep for 30 seconds\n",
    "    time.sleep(30)\n",
    "    print('Index Creation Completed:')  # Inform user that index creation is finished\n",
    "except RequestError as e:\n",
    "    print(f'Error while trying to create the index, with error {e.error}')\n",
    "    print('You may unmark the delete above to delete the existing index and recreate it')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da1be5-ae6d-4fcf-afe5-26c7fe6e7389",
   "metadata": {},
   "source": [
    "### Export variables to a file for the next lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7034600-6896-4954-9718-9112eedbab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"accountNumber\": account_number,\n",
    "            \"regionName\": region_name,\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"collectionId\": collection['createCollectionDetail']['id'],\n",
    "            \"vectorIndexName\": oss_index_name,\n",
    "            \"bedrockExecutionRoleArn\": bedrock_kb_execution_role_arn,\n",
    "            \"s3Bucket\": s3_bucket_name\n",
    "        }, f\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5284b9-3ed2-4003-862d-553542142e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
