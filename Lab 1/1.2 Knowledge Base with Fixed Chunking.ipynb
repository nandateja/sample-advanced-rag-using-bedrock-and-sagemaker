{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa74695-63c7-48a0-91b3-d3317035dd19",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with fixed chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9c5e0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we will implement a knowledge base using a fixed chunking strategy. Here are the key steps we'll perform:\n",
    "\n",
    "1. **Create a Knowledge Base**: Set up an Amazon Bedrock Knowledge Base with fixed-size chunking configuration that will store and retrieve our vector embeddings.\n",
    "\n",
    "2. **Create a Data Source**: Connect our Knowledge Base to the documents we uploaded to S3 in the previous notebook.\n",
    "\n",
    "3. **Start Ingestion Job**: Begin the process of transforming our documents into chunks, creating embeddings, and storing them in our vector database.\n",
    "\n",
    "4. **Retrieve and Generate**: Test our Knowledge Base by retrieving relevant information based on a sample query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce312491-c1c5-4754-89bd-d28da5558005",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "\n",
    "**Fixed Chunking**: Involves dividing your documents into fixed-size chunks, regardless of the content within them. Each chunk contains a predefined number of tokens or characters, and this method allows for more uniform data organization. \n",
    "\n",
    "Fixed chunking is useful when you want to ensure that your chunks are of a consistent size, making them easier to process and retrieve in a predictable manner. The document is split into sections of equal length, and each section becomes a separate chunk. This method works well when the content is relatively homogeneous, and the chunk boundaries are not as crucial to understanding the underlying context.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Uniformity**: Each chunk has the same size, making the system more predictable. This helps with processing efficiency since you know that each chunk is of a consistent size, making batch operations and parallel processing easier.\n",
    "- **Simplified Retrieval**: Since the chunk sizes are uniform, searching through the data becomes straightforward. You can quickly determine the length of chunks, which can be useful for performance optimization and scalability in large datasets.\n",
    "- **Performance Optimization**: Fixed chunks are ideal when you want to control the computational cost of document retrieval and chunking. Having equal-sized chunks reduces the chance of computational bottlenecks in scenarios requiring large-scale document processing.\n",
    "\n",
    "> **Note:** While fixed chunking can be efficient for certain use cases, it may not preserve the natural semantic boundaries of the content, such as paragraphs or sections. This may lead to chunks that start or end at arbitrary places, potentially cutting off context in the middle of a sentence or idea.\n",
    "\n",
    "### **Best Use Cases**\n",
    "Fixed chunking is suitable for cases where:\n",
    "- **Homogeneous content**: The content is consistent, and boundaries are not as important.\n",
    "- **Performance**: You need uniform-sized chunks for predictable processing or optimization of large-scale systems.\n",
    "- **Simplified text processing**: When chunk boundaries do not need to match natural semantic structures like paragraphs or sentences.\n",
    "\n",
    "Examples include:\n",
    "- **General document indexing**: When large datasets are involved, and uniform chunk sizes optimize retrieval.\n",
    "- **Text summarization**: Fixed chunking is helpful when generating summaries from uniformly sized data pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18e06c-98ef-41bc-99fd-d5e6c8977e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ceaa5e-2ddd-4b97-853f-527c92326ebd",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933328d-76dc-44bb-a985-591a61c47fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent client using the provided AWS region\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Retry decorator: If the function fails, it will retry up to 3 times with a random wait time between 1-2 seconds\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=3)\n",
    "def create_knowledge_base_func(name, description, chunking_type):\n",
    "    \"\"\"\n",
    "    Creates a knowledge base in Amazon Bedrock with OpenSearch Serverless as the vector store.\n",
    "    \n",
    "    Parameters:\n",
    "        name (str): The name of the knowledge base.\n",
    "        description (str): A description of the knowledge base.\n",
    "        chunking_type (str): The type of chunking strategy applied to vector indexing.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response containing details of the created knowledge base.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the ARN of the embedding model used for vectorization\n",
    "    embedding_model_arn = f\"arn:aws:bedrock:{variables['regionName']}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "    # Configure OpenSearch Serverless for vector storage\n",
    "    opensearch_serverless_configuration = {\n",
    "        \"collectionArn\": variables[\"collectionArn\"],  # ARN of the OpenSearch collection\n",
    "        \"vectorIndexName\": variables[\"vectorIndexName\"] + chunking_type,  # Index name based on chunking strategy\n",
    "        \"fieldMapping\": {  # Define field mappings for vectors, text, and metadata\n",
    "            \"vectorField\": \"vector\",\n",
    "            \"textField\": \"text\",\n",
    "            \"metadataField\": \"text-metadata\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(opensearch_serverless_configuration)  # Print configuration for debugging\n",
    "\n",
    "    # Create the knowledge base in Amazon Bedrock\n",
    "    create_kb_response = bedrock_agent.create_knowledge_base(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        roleArn=variables[\"bedrockExecutionRoleArn\"],  # IAM Role ARN for Bedrock execution\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embedding_model_arn  # Reference to the embedding model\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\": opensearch_serverless_configuration\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return create_kb_response[\"knowledgeBase\"]  # Return the created knowledge base details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68035c04-af12-4ae4-a48d-2e6391bb662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create a knowledge base using the predefined function\n",
    "kb = create_knowledge_base_func(\n",
    "    name=\"advanced-rag-workshop-fixed-chunking\",\n",
    "    description=\"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
    "    chunking_type=\"fixed\"\n",
    ")\n",
    "\n",
    "# Retrieve details of the newly created knowledge base\n",
    "get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "\n",
    "# Update the variables dictionary with the new knowledge base ID\n",
    "variables[\"kbFixedChunk\"] = kb['knowledgeBaseId']\n",
    "\n",
    "# Save updated variables to a JSON file, handling datetime serialization\n",
    "with open(\"variables.json\", \"w\") as f:\n",
    "    json.dump(variables, f, indent=4, default=str)  # Convert datetime to string\n",
    "\n",
    "# Print the retrieved knowledge base response in a readable format\n",
    "print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6a826-f3c8-40b5-aaa2-0c2a9662d5dc",
   "metadata": {},
   "source": [
    "### 2. Create Datasources for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb183c15-662d-498e-9df2-6814560f471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define the chunking strategy for data ingestion.\n",
    "# This specifies how the text will be divided into smaller chunks before storing in OpenSearch.\n",
    "chunking_strategy_configuration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",  # Use fixed-size chunks\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 1024,  # Maximum number of tokens per chunk\n",
    "        \"overlapPercentage\": 20  # Overlap percentage between consecutive chunks\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the S3 bucket configuration for the data source.\n",
    "# This tells Bedrock where to fetch the documents from.\n",
    "s3_configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{variables['s3Bucket']}\"  # S3 bucket ARN for document storage\n",
    "    # \"inclusionPrefixes\": [\"shareholder_letters\"]  # Uncomment to filter specific folder prefixes\n",
    "}\n",
    "\n",
    "# Check if a data source (`ds_fixed_chunk`) already exists in local variables.\n",
    "# If it exists, delete it before creating a new one.\n",
    "if 'ds_fixed_chunk' in locals():\n",
    "    try:\n",
    "        bedrock_agent.delete_data_source(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "            dataSourceId=ds_fixed_chunk[\"dataSourceId\"],  # ID of the existing data source\n",
    "        )\n",
    "        time.sleep(15)  # Wait for deletion to complete before proceeding\n",
    "    except Exception as e:\n",
    "        print(f\"Error while deleting existing data source: {e}\")\n",
    "        pass  # Continue execution even if deletion fails\n",
    "\n",
    "# Create a new data source in the Knowledge Base.\n",
    "create_ds_response = bedrock_agent.create_data_source(\n",
    "    name=\"advanced-rag-example\",  # Name of the data source\n",
    "    description=\"A data source for Advanced RAG workshop\",  # Description of the data source\n",
    "    knowledgeBaseId=kb['knowledgeBaseId'],  # Associate with the correct Knowledge Base\n",
    "    dataSourceConfiguration={\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\": s3_configuration  # Use the defined S3 configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration={\n",
    "        \"chunkingConfiguration\": chunking_strategy_configuration  # Define chunking settings\n",
    "    }\n",
    ")\n",
    "\n",
    "# Store the created data source object for future reference\n",
    "ds_fixed_chunk = create_ds_response[\"dataSource\"]\n",
    "\n",
    "# Print the newly created data source information\n",
    "ds_fixed_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c1ddb-cfdc-4c54-a688-033e6967073b",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97c8d9",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in the OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49134993-3ce8-4da2-a5a5-9917c974c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# List to keep track of all ingestion jobs\n",
    "ingest_jobs = []\n",
    "\n",
    "# Start an ingestion job for the data source\n",
    "try:\n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "        dataSourceId=ds_fixed_chunk[\"dataSourceId\"]  # ID of the associated data source\n",
    "    )\n",
    "    \n",
    "    # Extract job details\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(\"Ingestion job started successfully.\")\n",
    "\n",
    "    # Polling mechanism to check job status until it is complete\n",
    "    while job['status'] != 'COMPLETE':\n",
    "        time.sleep(30)  # Wait before checking the status again\n",
    "\n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base\n",
    "            dataSourceId=ds_fixed_chunk[\"dataSourceId\"],  # ID of the data source\n",
    "            ingestionJobId=job[\"ingestionJobId\"]  # ID of the running ingestion job\n",
    "        )\n",
    "        \n",
    "        # Update job status\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "        print(f\"Job status: {job['status']}\")  # Log the current job status\n",
    "\n",
    "    print(\"Ingestion job completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error: Couldn't start ingestion job.\")\n",
    "    print(e)  # Print the exact error message for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8b006-b8af-46d9-b869-77fb08a00ee0",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256881d-7737-4d21-a007-7959b22e8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query for retrieving relevant documents\n",
    "query = \"What are three sub-tasks in question answering over knowledge bases?\"\n",
    "\n",
    "try:\n",
    "    # Retrieve the top 3 most relevant documents from the knowledge base\n",
    "    relevant_documents_os = bedrock_agent_runtime.retrieve(\n",
    "        retrievalQuery={\n",
    "            'text': query  # Query text for document retrieval\n",
    "        },\n",
    "        knowledgeBaseId=kb['knowledgeBaseId'],  # ID of the Knowledge Base to search in\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': 3  # Fetch the top 3 most relevant documents\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print the retrieved documents for debugging\n",
    "    print(\"Successfully retrieved relevant documents.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error: Unable to retrieve relevant documents.\")\n",
    "    print(e)  # Print the error details for debugging\n",
    "\n",
    "# Output the retrieved documents\n",
    "relevant_documents_os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
