{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64457169-0b6e-411e-a1da-02c8a7d4ac8d",
   "metadata": {},
   "source": [
    "# Bedrock Knowledge Base Retrieval and Generation with Reranking\n",
    "\n",
    "The Rerank API in Amazon Bedrock is a new feature that improves the accuracy and relevance of responses in Retrieval-Augmented Generation (RAG) applications. It supports reranker models that rank a set of retrieved documents based on their relevance to a user's query, helping to prioritize the most relevant content for response generation.\n",
    "\n",
    "## Key features and use cases:\n",
    "\n",
    "1. **Enhancing RAG applications**: The Rerank API addresses challenges in semantic search, particularly with complex or ambiguous queries. For example, it can help a customer service chatbot focus on return policies rather than shipping guidelines when asked about returning an online purchase.\n",
    "\n",
    "2. **Improving search relevance**: It enables developers to significantly enhance their search relevance and content ranking capabilities, making enterprise-grade search technology more accessible.\n",
    "\n",
    "3. **Optimizing context window usage**: By ensuring the most useful information is sent to the foundation model, it potentially reduces costs and improves response accuracy.\n",
    "\n",
    "4. **Flexible integration**: The Rerank API can be used independently to rerank documents even if you're not using Amazon Bedrock Knowledge Bases.\n",
    "\n",
    "5. **Multiple model support**: At launch, it supports Amazon Rerank 1.0 and Cohere Rerank 3.5 models.\n",
    "\n",
    "6. **Customizable configurations**: Developers can specify additional model configurations as key-value pairs for more tailored reranking.\n",
    "\n",
    "The Rerank API is available in select AWS Regions, including US West (Oregon), Canada (Central), Europe (Frankfurt), and Asia Pacific (Tokyo). It can be integrated into existing systems at scale, whether keyword-based or semantic, through a single API call in Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3958d",
   "metadata": {},
   "source": [
    "![Reranking](./reranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ecae6-7c0a-4490-857c-c41e5c8d445e",
   "metadata": {},
   "source": [
    "## 1: Import and Load Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a1a59-a87c-4953-9b79-1194f7fcce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the configuration variables from a JSON file\n",
    "with open(\"../Lab 1/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c78a0f-b980-4753-b2e0-5dcd0b0f44cb",
   "metadata": {},
   "source": [
    "## 2: Define ARN and Configuration Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85280494-5d51-4c3b-939c-b4f482e422c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up configuration for Bedrock\n",
    "regionName=variables['regionName'] \n",
    "accountNumber = variables['accountNumber']\n",
    "knowledge_base_id = variables['kbSemanticChunk']\n",
    "model_id = 'us.amazon.nova-pro-v1:0' \n",
    "\n",
    "# Define ARNs (Amazon Resource Names) for the model\n",
    "model_arn = f\"arn:aws:bedrock:us-west-2:{accountNumber}:inference-profile/{model_id}\"\n",
    "rerank_model_arn=f\"arn:aws:bedrock:us-west-2::foundation-model/cohere.rerank-v3-5:0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b99bb-d6c8-4283-bdac-51919d025d45",
   "metadata": {},
   "source": [
    "## 3: Set Up Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80f441-1cd3-4ea0-8d04-5043c2a8ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import *\n",
    "\n",
    "# Configure the Bedrock client\n",
    "bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name=regionName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53cec78-9627-4ab3-a260-bb8098ebd22a",
   "metadata": {},
   "source": [
    "## 4: Function to use Bedrock Converse API to send tokens to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff456bf-4180-4218-95f5-058acd5bf6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_value_by_key_path(d, path):\n",
    "    \"\"\"\n",
    "    Retrieve a value from a nested dictionary using a key path.\n",
    "\n",
    "    Args:\n",
    "        d (dict): The dictionary to search.\n",
    "        path (list): List of keys forming the path to the desired value.\n",
    "\n",
    "    Returns:\n",
    "        The value at the specified path, or None if not found.\n",
    "    \"\"\"\n",
    "    current = d\n",
    "    for key in path:\n",
    "        try:\n",
    "            current = current[key]\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            return None  # Return None if the path is invalid (key not found, wrong type, etc.)\n",
    "    return current\n",
    "\n",
    "def invoke_converse(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    model_id: str,\n",
    "    temperature: float = 0.1,\n",
    "    max_tokens: int = 4000\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Chat with a Bedrock model using the Converse API.\n",
    "    \n",
    "    Args:\n",
    "        system_prompt (str): System instructions/context\n",
    "        user_prompt (str): User's input/question\n",
    "        model_id (str): Bedrock model ID\n",
    "        temperature (float): Controls randomness (0.0 to 1.0)\n",
    "        max_tokens (int): Maximum tokens in response\n",
    "        \n",
    "    Returns:\n",
    "        Optional[str]: Model's response or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Bedrock Runtime client with configuration\n",
    "        client = boto3.client('bedrock-runtime', region_name=regionName)\n",
    "        \n",
    "        # Prepare the system prompt from session state\n",
    "        system_prompt = [{'text': system_prompt}]\n",
    "        messages = []\n",
    "\n",
    "        # Format the user's question as a message\n",
    "        message = {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [            \n",
    "                {\n",
    "                    \"text\": f\"{user_prompt}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Set inference configuration\n",
    "        messages.append(message)\n",
    "        inferenceConfig = {\n",
    "            \"maxTokens\" : 4096,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        #invoke the API\n",
    "        answer = \"\"\n",
    "        response = client.converse(modelId=model_id, \n",
    "                                messages=messages,\n",
    "                                system=system_prompt,\n",
    "                                inferenceConfig = inferenceConfig)\n",
    "        \n",
    "        # Process the response\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200 :\n",
    "            # Extract and concatenate the content from the response \n",
    "            content_list = get_value_by_key_path(response, ['output', 'message', 'content'])\n",
    "            # print(f\"content_list = {content_list}\")\n",
    "            answer = \"\"\n",
    "            for content in content_list :\n",
    "                text = content.get('text')\n",
    "                if text:  # Only concatenate if text is not None\n",
    "                    answer += text\n",
    "        else :\n",
    "            # Format an error message if the request was unsuccessful\n",
    "            answer = f\"Error: {response['ResponseMetadata']['HTTPStatusCode']} - {response['Error']['Message']}\"\n",
    "        return answer, response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in invoke_converse: {str(e)}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452523ec-b019-4030-a164-2d309401e619",
   "metadata": {},
   "source": [
    "## 5: Function to search Knowledge Base (vector database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9f078-585e-4d8e-96c1-afc1c722bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def search_kb(query, kb_id, numberOfResults):\n",
    "    \"\"\"Use query to search the knowledge base with the guve kb_id for the specified number of search results.\"\"\"\n",
    "\n",
    "    # Create the client\n",
    "    client = boto3.client(\"bedrock-agent-runtime\", region_name=regionName)\n",
    "    \n",
    "    # Retrieve from knowledge base\n",
    "    kb_response = client.retrieve(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalQuery={\"text\": query},\n",
    "        retrievalConfiguration={\"vectorSearchConfiguration\": {\"numberOfResults\": numberOfResults}}\n",
    "    )\n",
    "    \n",
    "    # Extract documents\n",
    "    documents = []\n",
    "    original_results = []\n",
    "    \n",
    "    for i, result in enumerate(kb_response.get(\"retrievalResults\", [])):\n",
    "        # Extract text from result\n",
    "        text = \"\"\n",
    "        if \"content\" in result and \"text\" in result[\"content\"]:\n",
    "            text = \"\".join([item.get(\"span\", \"\") if isinstance(item, dict) else str(item) \n",
    "                           for item in result[\"content\"][\"text\"]])\n",
    "            \n",
    "        # Store original result\n",
    "        original_results.append({\n",
    "            \"position\": i + 1,\n",
    "            \"score\": result.get(\"scoreValue\", 0),\n",
    "            \"text\": text\n",
    "            # \"text\": text[:300] + \"...\" if len(text) > 300 else text\n",
    "        })\n",
    "        documents.append(text)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e95426-0f43-4f3e-a49f-13c756c163d6",
   "metadata": {},
   "source": [
    "## 5: Get results from KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692b96c-810a-40b0-9e1e-8ccf6b90a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is red teaming? How can it be used with text to SQL?\"\n",
    "\n",
    "numberOfResults = 5\n",
    "original_kb_results = search_kb(query, knowledge_base_id, numberOfResults)\n",
    "kb_context = '\\n\\n'.join(original_kb_results)\n",
    "# print(json.dumps(kb_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a0bfd-7322-47a0-8af9-7904acb3dee6",
   "metadata": {},
   "source": [
    "## 6: Get response from the LLM\n",
    "We will use the results we recieve from KnowledgeBase (KB) as-is. We are not doing any re ranking yet.\n",
    "We will send the context from KB and the user query to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288e51c-0f49-4c82-ae74-0d61ec9527fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'us.amazon.nova-pro-v1:0' \n",
    "\n",
    "#Create a good system prompt that will ask the LLM to behave in a certain way.\n",
    "system_prompt = f\"\"\"\n",
    "Please use the context below to respond to the question. \n",
    "If you have enough information to answer the question, please explain the reasoning behind the response.\n",
    "If you do not have enough information to answer the question, please don't guess. Instead, just say I don't know with the reason.\n",
    "CONTEXT:\n",
    "{kb_context}\n",
    "\"\"\"\n",
    "# We will send this query to the LLM. Red Teaming and text-to-SQL are in two different documents. \n",
    "# With fewer chunks like 5 from KB and no reranking, the results from KB will not be as relevant. We should expect an I don't know answer.\n",
    "query = \"What is red teaming? How can it be used with text to SQL?\"\n",
    "\n",
    "#Send the system prompt, context from KB, user query to the LLM.\n",
    "answer, response = invoke_converse(system_prompt, query, model_id)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f4592-a99b-4e20-ab7b-6776053fbc2e",
   "metadata": {},
   "source": [
    "## 7: Function to Rerank Chunks recieved from KB\n",
    "Reranking works by sending the original query and the results to a ReRanking API.\n",
    "AWS offers two Reranking APIs:  Amazon Rerank, and Cohere Rerank.\n",
    "In this exercise, we will use Cohere Rerank API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044069d-8f2e-403d-961a-4c2faf665ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query, kb_results, rerank_model_arn, reranked_result_count = 5) :\n",
    "\n",
    "    # Create the client\n",
    "    client = boto3.client(\"bedrock-agent-runtime\", region_name=regionName)\n",
    "\n",
    "    #invoke the rerank API\n",
    "    reranked = client.rerank(\n",
    "        queries=[{\"textQuery\": {\"text\": query}, \"type\": \"TEXT\"}],\n",
    "        rerankingConfiguration={\n",
    "            \"bedrockRerankingConfiguration\": {\n",
    "                \"modelConfiguration\": {\"modelArn\": rerank_model_arn},\n",
    "                \"numberOfResults\": reranked_result_count\n",
    "            },\n",
    "            \"type\": \"BEDROCK_RERANKING_MODEL\"\n",
    "        },\n",
    "        sources=[{\n",
    "            \"inlineDocumentSource\": {\"textDocument\": {\"text\": doc}, \"type\": \"TEXT\"},\n",
    "            \"type\": \"INLINE\"\n",
    "        } for doc in kb_results]\n",
    "    )\n",
    "    \n",
    "    # Process reranked results\n",
    "    reranked_results = []\n",
    "    for result in reranked.get(\"results\", []):\n",
    "        idx = result.get(\"index\", 0)\n",
    "        reranked_results.append({\n",
    "            \"original_position\": idx + 1,\n",
    "            \"new_position\": len(reranked_results) + 1,\n",
    "            \"relevance_score\": result.get(\"relevanceScore\", 0),  # Full precision score\n",
    "            \"text\": kb_results[idx]\n",
    "        })\n",
    "    return {\"original_results\": kb_results, \"reranked_results\": reranked_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59048571-f4f3-4667-95be-82c77bb98242",
   "metadata": {},
   "source": [
    "## 8: Use Rerank API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b09d8-8ce9-4664-822e-ed5b67c5e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get more results from KB and then rerank and reduce the context before sending to LLM.\n",
    "numberOfResults = 20\n",
    "original_kb_results = search_kb(query, knowledge_base_id, numberOfResults)\n",
    "# print(json.dumps(kb_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058dede6-07cf-4fd9-b604-9caab89a3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the results from KB search to Rerank API and get a smaller count of results such as 5 (instead of all 20). \n",
    "# This helps eliminate sending lower ranked results to the LLM. Also reduces input tokens and hence reduces costs and latency as well.\n",
    "reranked_result_count = 5\n",
    "reranked_json = rerank_results(query, original_kb_results, rerank_model_arn, reranked_result_count)\n",
    "# print(json.dumps(reranked_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c500b-5ba9-46b6-9cff-d7d51f574fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's concatenate the reranked results into a string so that we can send the string to the LLM as context.\n",
    "kb_context = \"\"\n",
    "for result in reranked_json['reranked_results'] :\n",
    "    kb_context += result['text'] + \"\\n\\n\"\n",
    "# kb_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9979067-1138-4f75-b745-f53e629ce89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'us.amazon.nova-pro-v1:0' \n",
    "\n",
    "#We will send the same system prompt. The only change is the re ranked context.\n",
    "system_prompt = f\"\"\"\n",
    "Please use the context below to respond to the question. \n",
    "If you have enough information to answer the question, please explain the reasoning behind the response.\n",
    "If you do not have enough information to answer the question, please don't guess. Instead, just say I don't know with the reason.\n",
    "CONTEXT:\n",
    "{kb_context}\n",
    "\"\"\"\n",
    "\n",
    "# We will send the same query to the LLM.\n",
    "query = \"What is red teaming? How can it be used with text to SQL?\"\n",
    "\n",
    "# Send the system prompt, context from KB, user query to the LLM. We should see a better response because we re ranked the context from KB.\n",
    "answer, result = invoke_converse(system_prompt, query, model_id)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161508b1-2db4-483a-bb5d-863a6347d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are you curious how to find input tokens, output tokens, and latency?\n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
