{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Semantic Chunking KB with multiple models for RAG\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook demonstrates Retrieval-Augmented Generation (RAG) using a predefined ground truth to evaluate the effectiveness of multiple models on semantic chunking knowledge base (created in Lab 1). We utilize Amazon Nova lite, Nova micro, Claude Haiku and Claude Sonnet for generating responses and FloTorch for evaluating these responses against the ground truth after retrieving information from the respective knowledge bases.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Ensure that all prerequisites outlined in the `4.1 Prerequisites.ipynb` notebook have been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load prompt.json\n",
    "\n",
    "prompt.json file includes the following:\n",
    "\n",
    "* system_prompt\n",
    "* examples for n shot learning\n",
    "* user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './data/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the evaluation against Semantic Chunking KB\n",
    "\n",
    "**Important:** This step assumes that your knowledge base has already been created in Lab 1. Please ensure that you have completed the knowledge base creation as part of Lab 1 before proceeding.\n",
    "\n",
    "Inference Models considered - Amazon Nova Lite, Amazon Nova Pro, Claude Haiku 3.5, Claude Sonnet 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_kb = variables['kbSemanticChunk']\n",
    "\n",
    "inference_models = [\"us.amazon.nova-lite-v1:0\",\"us.amazon.nova-micro-v1:0\",\n",
    "                \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Config\n",
    "\n",
    "* **Bedrock KB Id:** KnowledgeBase Id to query against\n",
    "* **KNN (k-Nearest Neighbors):** 5\n",
    "* **Rerank Model:** Amazon Rerank\n",
    "* **N-Shot Prompt:** 1\n",
    "* **Temperature:** 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "    \"bedrock_kb_id\": bedrock_kb,\n",
    "    \"temp_retrieval_llm\": \"0.1\",\n",
    "    \"gt_data\": variables[\"s3_ground_truth_path\"],\n",
    "    \"rerank_model_id\": \"amazon.rerank-v1:0\",\n",
    "    \"retrieval_service\": \"bedrock\",\n",
    "    \"knn_num\": \"5\",\n",
    "    \"retrieval_model\": \"us.amazon.nova-lite-v1:0\",\n",
    "    \"aws_region\": variables['regionName'],\n",
    "    \"n_shot_prompt_guide_obj\": prompt,\n",
    "    \"n_shot_prompts\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ground truth data\n",
    "\n",
    "We utilize FloTorch core's S3StorageProvider and JSONReader to load ground truth data for evaluating the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_rag_utils import Question\n",
    "\n",
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)\n",
    "questions = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Vector Storage\n",
    "\n",
    "The `VectorStorageFactory` serves as a central factory for creating and managing various vector storage implementations. It instantiates the appropriate backend based on the provided configuration parameters.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "* `knowledge_base` (*bool*): A boolean flag indicating whether vector storage is enabled.\n",
    "* `use_bedrock_kb` (*bool*): Determines the vector storage implementation to use:\n",
    "    * `True`: Utilizes Bedrock Knowledge Base.\n",
    "    * `False`: Utilizes a provisioned OpenSearch cluster.\n",
    "* `embedding` (*BaseEmbedding*): An instance of the embedding model to be used for generating vector embeddings.\n",
    "* `opensearch_host` (*Optional[str]*): The hostname or IP address of the OpenSearch server. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `opensearch_port` (*Optional[int]*): The port number for connecting to the OpenSearch server. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `opensearch_username` (*Optional[str]*): The username for OpenSearch authentication. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `opensearch_password` (*Optional[str]*): The password for OpenSearch authentication. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `index_id` (*Optional[str]*): The identifier for the OpenSearch index. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `knowledge_base_id` (*Optional[str]*): The identifier for the Bedrock Knowledge Base. **Required only when `use_bedrock_kb` is `True`.**\n",
    "* `aws_region` (*str*): The AWS region for Bedrock Knowledge Base operations. Defaults to `\"us-east-1\"`. **Required only when `use_bedrock_kb` is `True`.**\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "(*VectorStorage*): An instance of either `OpenSearchClient` or `BedrockKnowledgeBaseStorage` based on the value of the `use_bedrock_kb` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "\n",
    "vector_storage = VectorStorageFactory.create_vector_storage(\n",
    "        knowledge_base=True,\n",
    "        use_bedrock_kb=True,\n",
    "        embedding=None,\n",
    "        knowledge_base_id=exp_config_data.get(\"bedrock_kb_id\"),\n",
    "        aws_region=exp_config_data.get(\"aws_region\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize BedrockReranker\n",
    "\n",
    "The `BedrockReranker` class enables the reranking of documents using reranking models available through Amazon Bedrock. This is beneficial for refining the order of retrieved documents to enhance their relevance to a given query.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "* `region` (*str*): The AWS region where the Bedrock service is accessible.\n",
    "* `rerank_model_id` (*str*): The identifier of the specific Bedrock reranking model to be used.\n",
    "* `bedrock_client` (*Optional[boto3.client]*): An optional, pre-configured Bedrock agent runtime client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.rerank.rerank import BedrockReranker\n",
    "\n",
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "    if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "    else None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Bedrock Inferencer\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on service and the model \n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* – Enables FloTorch LLM gateway-based invocation if set to `True`.\n",
    "- `gateway_url`: *(str)* – URL endpoint for the FloTorch LLM Gateway.\n",
    "- `gateway_api_key`: *(str)* – API key for authenticating requests to the FloTorch LLM gateway.\n",
    "- `retrieval_service`: *(str)* – Name of the retrieval service (e.g., bedrock, sagemaker).\n",
    "- `retrieval_model`: *(str)* – The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `aws_region`: *(str)* – AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `iam_role`: *(str)* – IAM role ARN for SageMaker invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* – Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* – Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* – Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "#### Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the FloTorch LLM Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock or AWS SageMaker.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "#### Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "\n",
    "def initialize_inferencer(inference_model: str, exp_config_data: dict):\n",
    "    inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "        False,\"\",\"\",\n",
    "        exp_config_data.get(\"retrieval_service\"),\n",
    "        inference_model, \n",
    "        exp_config_data.get(\"aws_region\"), \n",
    "        variables['bedrockExecutionRoleArn'],\n",
    "        int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "        float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "        exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    "    )\n",
    "    return inferencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute RAG against all the inference models\n",
    "\n",
    "Perform the retrieval, reranking, and inference steps using the `flotorch-core` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_rag_utils import rag_with_flotorch\n",
    "\n",
    "rag_response_dict = {}\n",
    "\n",
    "# The evaluation process duration is dependent on the volume of questions and the number of models bases being evaluated. \n",
    "# Larger evaluations require more time, generally around 5-6 minutes.\n",
    "for inference_model in inference_models:\n",
    "    inferencer = initialize_inferencer(inference_model, exp_config_data)\n",
    "    responses = rag_with_flotorch(exp_config_data, vector_storage, reranker, inferencer, questions)\n",
    "    rag_response_dict[inference_model] = responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the results to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"./results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "# Save to JSON with proper formatting\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rag_response_dict, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
