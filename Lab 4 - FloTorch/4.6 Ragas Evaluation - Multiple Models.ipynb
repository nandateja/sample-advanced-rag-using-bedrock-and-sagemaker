{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic KB: Evaluating Multiple Models with FloTorch\n",
    "\n",
    "[FloTorch](https://www.flotorch.ai/) provides a powerful framework for evaluating Retrieval-Augmented Generation (RAG) systems, allowing for in-depth assessment and comparison. It emphasizes key metrics like accuracy, cost, and latency, which are critical for RAG pipeline assessment.\n",
    "\n",
    "### Key Evaluation Metrics for this Notebook\n",
    "\n",
    "This notebook will focus on evaluating our RAG pipelines using the following metrics:\n",
    "\n",
    "* **Context Precision:** Measures the relevance of the retrieved context chunks. It's the average of the precision@k scores for each chunk in the retrieved context. Precision@k is the proportion of relevant chunks within the top k retrieved chunks.\n",
    "\n",
    "* **Faithfulness:** Quantifies how factually consistent the generated response is with the retrieved context. Scores range from 0 to 1, with higher values indicating greater consistency. A response is considered faithful if all its claims are supported by the retrieved context.\n",
    "\n",
    "* **Response Relevancy:** Assesses how well the generated response addresses the user's query. Higher scores indicate better relevance and completeness, while lower scores suggest incompleteness or the inclusion of irrelevant information.\n",
    "\n",
    "* **Inference Cost:** The total cost incurred for using Bedrock models to generate responses for all questions in the ground truth dataset.\n",
    "\n",
    "* **Latency:** The time taken for the inference process, specifically the duration of Bedrock model invocations.\n",
    "\n",
    "### Leveraging Ragas for Evaluation\n",
    "\n",
    "This evaluation process utilizes [Ragas](https://docs.ragas.io/en/stable/), a library designed to simplify and enhance the evaluation of Large Language Model (LLM) applications, enabling confident and straightforward assessment.\n",
    "\n",
    "Internally, Ragas uses Large Language Models (LLMs) to calculate both Context Precision and Response Relevancy scores. In this evaluation, we will specifically use `amazon.titan-embed-text-v2` for generating embeddings and `amazon.nova-pro-v1:0` for the inference tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Config\n",
    "\n",
    "We will evaluate the RAG pipeline using Amazon Nova Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_config_data = {\n",
    "   \"eval_embedding_model\" : \"amazon.titan-embed-text-v2:0\",\n",
    "   \"eval_retrieval_model\" : \"us.amazon.nova-pro-v1:0\",\n",
    "   \"eval_retrieval_service\" : \"bedrock\",\n",
    "   \"aws_region\" : variables['regionName'],\n",
    "   \"eval_embed_vector_dimension\" : 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAG response data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from evaluation_utils import convert_to_evaluation_dict\n",
    "\n",
    "filename = f\"./results/ragas_evaluation_responses_for_different_models.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    loaded_responses = json.load(f)\n",
    "\n",
    "evaluation_dataset_per_model = convert_to_evaluation_dict(loaded_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evaluation = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Evaluation with Ragas\n",
    "\n",
    "**Important Note on Metric Variability:** Due to the inherent stochasticity of Large Language Models (LLMs), the sensitivity of evaluation metrics, and the quality of the LLM used, Ragas metrics for the same dataset can vary between evaluations. While efforts are made to improve reproducibility, please be aware that fluctuations in evaluation scores are possible.\n",
    "\n",
    "For more information, refer to this GitHub issue: [https://github.com/explodinggradients/ragas/issues/1125](https://github.com/explodinggradients/ragas/issues/1125)\n",
    "\n",
    "**Understanding NaN Values in Evaluation Results:**\n",
    "\n",
    "You might encounter `NaN` (Not a Number) values in the evaluation results. This typically occurs for two primary reasons:\n",
    "\n",
    "1.  **JSON Parsing Issue:** Ragas expects LLM outputs to be in a JSON-parsable format because its prompts are structured using Pydantic. This ensures efficient processing of the model's responses. If the model's output is not valid JSON, `NaN` may appear.\n",
    "\n",
    "2.  **Non-Ideal Cases for Scoring:** Certain scenarios within the evaluation dataset might not be suitable for calculating specific metrics. For instance, assessing the faithfulness of a response like \"I donâ€™t know\" might not be meaningful, leading to a `NaN` value for that metric in such cases.\n",
    "\n",
    "For further details, please consult the Ragas documentation: [https://github.com/explodinggradients/ragas/blob/main/docs/index.md#frequently-asked-questions](https://github.com/explodinggradients/ragas/blob/main/docs/index.md#frequently-asked-questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.embedding.embedding_registry import embedding_registry\n",
    "from flotorch_core.embedding.titanv2_embedding import TitanV2Embedding\n",
    "from flotorch_core.embedding.cohere_embedding import CohereEmbedding\n",
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "from flotorch_core.evaluator.ragas_evaluator import RagasEvaluator\n",
    "\n",
    "# Initialize embeddings\n",
    "embedding_class = embedding_registry.get_model(evaluation_config_data.get(\"eval_embedding_model\"))\n",
    "embedding = embedding_class(evaluation_config_data.get(\"eval_embedding_model\"), \n",
    "                            evaluation_config_data.get(\"aws_region\"), \n",
    "                            int(evaluation_config_data.get(\"eval_embed_vector_dimension\"))\n",
    "                            )\n",
    "\n",
    "# Initialize inferencer\n",
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "    False,\"\",\"\",\n",
    "    evaluation_config_data.get(\"eval_retrieval_service\"),\n",
    "    evaluation_config_data.get(\"eval_retrieval_model\"), \n",
    "    evaluation_config_data.get(\"aws_region\"), \n",
    "    variables['bedrockExecutionRoleArn'],\n",
    "    float(0.1)\n",
    ")\n",
    "\n",
    "evaluator = RagasEvaluator(inferencer, embedding)\n",
    "\n",
    "for model in evaluation_dataset_per_model:\n",
    "    # You might encounter some warnings and errors on the console - please ignore them\n",
    "    # Those are ragas errors and it shouldn't impact our flow\n",
    "    ragas_report = evaluator.evaluate(evaluation_dataset_per_model[model])\n",
    "    if ragas_report:\n",
    "        eval_metrics = ragas_report._repr_dict\n",
    "        eval_metrics = {key: round(value, 2) if isinstance(value, float) else value for key, value in eval_metrics.items()} \n",
    "    final_evaluation[model] = {\n",
    "            'llm_context_precision_with_reference': eval_metrics['llm_context_precision_with_reference'],\n",
    "            'faithfulness': eval_metrics['faithfulness'],\n",
    "            'answer_relevancy': eval_metrics['answer_relevancy']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Latency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cost_compute_utils import calculate_cost_and_latency_metrics\n",
    "\n",
    "for model in loaded_responses:\n",
    "    inference_data = loaded_responses[model]\n",
    "    cost_and_latency_metrics = calculate_cost_and_latency_metrics(inference_data, model,\n",
    "                evaluation_config_data[\"aws_region\"])\n",
    "    \n",
    "    if model not in final_evaluation:\n",
    "        # Insert - key doesn't exist yet\n",
    "        final_evaluation[model] = cost_and_latency_metrics\n",
    "    else:\n",
    "        # Update - key already exists\n",
    "        final_evaluation[model].update(cost_and_latency_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the nested dictionary to a DataFrame\n",
    "evaluation_df = pd.DataFrame.from_dict(final_evaluation, orient='index')\n",
    "\n",
    "# If you want the kb_type as a column instead of an index\n",
    "evaluation_df = evaluation_df.reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlab_utils import plot_grouped_bar\n",
    "\n",
    "plot_grouped_bar(evaluation_df, 'model', ['llm_context_precision_with_reference', 'faithfulness', 'answer_relevancy'], show_values=True, title='Evaluation Metrics', xlabel='KB Type', ylabel='Metrics')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
