{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Fixed vs. Semantic Chunking in Knowledge Bases for RAG\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook demonstrates Retrieval-Augmented Generation (RAG) using a predefined ground truth to evaluate the effectiveness of fixed and semantic chunking knowledge bases (created in Lab 1). We utilize the Amazon Bedrock Nova lite model for generating responses and FloTorch for evaluating these responses against the ground truth after retrieving information from the respective knowledge bases.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Ensure that all prerequisites outlined in the `4.1 Prerequisites.ipynb` notebook have been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load prompt.json\n",
    "\n",
    "prompt.json file includes the following:\n",
    "\n",
    "* system_prompt\n",
    "* examples for n shot learning\n",
    "* user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_path = './data/prompt.json'\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Bedrock Knowledge Base IDs\n",
    "\n",
    "This step fetches the IDs of all Bedrock knowledge bases created during Lab 1. It assumes that knowledge bases using both fixed and semantic chunking methods exist. Furthermore, if knowledge bases employing hierarchical or custom chunking were also created, this notebook will also be able to include them in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_kbs = {}\n",
    "\n",
    "kb_types = ['kbFixedChunk', 'kbSemanticChunk', 'kbHierarchicalChunk', 'kbCustomChunk']\n",
    "bedrock_kbs = {variables[kb_type]:kb_type for kb_type in kb_types if variables.get(kb_type)}\n",
    "\n",
    "bedrock_kbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Config\n",
    "\n",
    "We will use the following fixed parameters for retrieval and inference:\n",
    "\n",
    "* **KNN (k-Nearest Neighbors):** 5\n",
    "* **Rerank Model:** Amazon Rerank\n",
    "* **N-Shot Prompt:** 1\n",
    "* **Inference Model:** Nova-Lite (via Bedrock)\n",
    "* **Temperature:** 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config_data = {\n",
    "    \"temp_retrieval_llm\": \"0.1\",\n",
    "    \"gt_data\": variables[\"s3_ground_truth_path\"],\n",
    "    \"rerank_model_id\": \"amazon.rerank-v1:0\",\n",
    "    \"retrieval_service\": \"bedrock\",\n",
    "    \"knn_num\": \"5\",\n",
    "    \"retrieval_model\": \"us.amazon.nova-lite-v1:0\",\n",
    "    \"aws_region\": variables['regionName'],\n",
    "    \"n_shot_prompt_guide_obj\": prompt,\n",
    "    \"n_shot_prompts\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ground truth data\n",
    "\n",
    "We utilize FloTorch core's S3StorageProvider and JSONReader to load ground truth data for evaluating the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.storage_provider_factory import StorageProviderFactory\n",
    "from flotorch_core.reader.json_reader import JSONReader\n",
    "from flotorch_rag_utils import Question\n",
    "\n",
    "gt_data = exp_config_data['gt_data']\n",
    "storage = StorageProviderFactory.create_storage_provider(gt_data)\n",
    "gt_data_path = storage.get_path(gt_data)\n",
    "json_reader = JSONReader(storage)\n",
    "questions = json_reader.read_as_model(gt_data_path, Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Vector Storage\n",
    "\n",
    "The `VectorStorageFactory` serves as a central factory for creating and managing various vector storage implementations. It instantiates the appropriate backend based on the provided configuration parameters.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "* `knowledge_base` (*bool*): A boolean flag indicating whether vector storage is enabled.\n",
    "* `use_bedrock_kb` (*bool*): Determines the vector storage implementation to use:\n",
    "    * `True`: Utilizes Bedrock Knowledge Base.\n",
    "    * `False`: Utilizes a provisioned OpenSearch cluster.\n",
    "* `embedding` (*BaseEmbedding*): An instance of the embedding model to be used for generating vector embeddings.\n",
    "* `opensearch_host` (*Optional[str]*): The hostname or IP address of the OpenSearch server. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `opensearch_port` (*Optional[int]*): The port number for connecting to the OpenSearch server. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `opensearch_username` (*Optional[str]*): The username for OpenSearch authentication. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `opensearch_password` (*Optional[str]*): The password for OpenSearch authentication. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `index_id` (*Optional[str]*): The identifier for the OpenSearch index. **Required only when `use_bedrock_kb` is `False`.**\n",
    "* `knowledge_base_id` (*Optional[str]*): The identifier for the Bedrock Knowledge Base. **Required only when `use_bedrock_kb` is `True`.**\n",
    "* `aws_region` (*str*): The AWS region for Bedrock Knowledge Base operations. Defaults to `\"us-east-1\"`. **Required only when `use_bedrock_kb` is `True`.**\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "(*VectorStorage*): An instance of either `OpenSearchClient` or `BedrockKnowledgeBaseStorage` based on the value of the `use_bedrock_kb` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.storage.db.vector.vector_storage_factory import VectorStorageFactory\n",
    "\n",
    "def vector_storage(bedrock_kb_id: str, exp_config_data: dict):\n",
    "    vector_storage = VectorStorageFactory.create_vector_storage(\n",
    "            knowledge_base=True,\n",
    "            use_bedrock_kb=True,\n",
    "            embedding=None,\n",
    "            knowledge_base_id=bedrock_kb_id,\n",
    "            aws_region=exp_config_data.get(\"aws_region\")\n",
    "        )\n",
    "    return vector_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize BedrockReranker\n",
    "\n",
    "The `BedrockReranker` class enables the reranking of documents using reranking models available through Amazon Bedrock. This is beneficial for refining the order of retrieved documents to enhance their relevance to a given query.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "* `region` (*str*): The AWS region where the Bedrock service is accessible.\n",
    "* `rerank_model_id` (*str*): The identifier of the specific Bedrock reranking model to be used.\n",
    "* `bedrock_client` (*Optional[boto3.client]*): An optional, pre-configured Bedrock agent runtime client.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.rerank.rerank import BedrockReranker\n",
    "\n",
    "reranker = BedrockReranker(exp_config_data.get(\"aws_region\"), exp_config_data.get(\"rerank_model_id\")) \\\n",
    "    if exp_config_data.get(\"rerank_model_id\").lower() != \"none\" \\\n",
    "    else None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Bedrock Inferencer\n",
    "\n",
    "Creates and returns an appropriate `Inferencer` instance depending on service and the model \n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- `gateway_enabled`: *(bool)* – Enables FloTorch LLM gateway-based invocation if set to `True`.\n",
    "- `gateway_url`: *(str)* – URL endpoint for the FloTorch LLM Gateway.\n",
    "- `gateway_api_key`: *(str)* – API key for authenticating requests to the FloTorch LLM gateway.\n",
    "- `retrieval_service`: *(str)* – Name of the retrieval service (e.g., bedrock, sagemaker).\n",
    "- `retrieval_model`: *(str)* – The model to use for inference (e.g., `anthropic.claude-v2`).\n",
    "- `aws_region`: *(str)* – AWS region for service provisioning (e.g., `us-east-1`).\n",
    "- `iam_role`: *(str)* – IAM role ARN for SageMaker invocation permissions.\n",
    "- `n_shot_prompts`: *(int)* – Number of few-shot examples to include in prompt.\n",
    "- `temp_retrieval_llm`: *(float)* – Temperature setting for the language model.\n",
    "- `n_shot_prompt_guide_obj`: *(Any)* – Few-shot guide object for prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "#### Behavior\n",
    "\n",
    "- If `gateway_enabled` is `True`, connects to the FloTorch LLM Gateway using credentials.\n",
    "- If disabled, falls back to direct model invocation through supported services like AWS Bedrock or AWS SageMaker.\n",
    "- Supports dynamic few-shot prompting and custom temperature configuration.\n",
    "\n",
    "---\n",
    "\n",
    "#### Outcome\n",
    "\n",
    "Returns a fully configured `Inferencer` object capable of generating answers or completions for queries using the selected language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flotorch_core.inferencer.inferencer_provider_factory import InferencerProviderFactory\n",
    "\n",
    "inferencer = InferencerProviderFactory.create_inferencer_provider(\n",
    "    False,\"\",\"\",\n",
    "    exp_config_data.get(\"retrieval_service\"),\n",
    "    exp_config_data.get(\"retrieval_model\"), \n",
    "    exp_config_data.get(\"aws_region\"), \n",
    "    variables['bedrockExecutionRoleArn'],\n",
    "    int(exp_config_data.get(\"n_shot_prompts\", 0)), \n",
    "    float(exp_config_data.get(\"temp_retrieval_llm\", 0)), \n",
    "    exp_config_data.get(\"n_shot_prompt_guide_obj\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute RAG on All Available Bedrock Knowledge Bases\n",
    "\n",
    "Perform the retrieval, reranking, and inference steps using the `flotorch-core` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from flotorch_rag_utils import rag_with_flotorch\n",
    "\n",
    "rag_response_dict = {}\n",
    "\n",
    "# The evaluation process duration is dependent on the volume of questions and the number of knowledge bases being evaluated. \n",
    "# Larger evaluations require more time, generally around 5-6 minutes.\n",
    "for bedrock_kb_id in bedrock_kbs:\n",
    "    vector_storage(bedrock_kb_id, exp_config_data)\n",
    "    responses = rag_with_flotorch(exp_config_data, vector_storage, reranker, inferencer, questions)\n",
    "    rag_response_dict[bedrock_kbs[bedrock_kb_id]] = responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the results to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"./results/ragas_evaluation_responses_for_different_kbs.json\"\n",
    "\n",
    "# Save to JSON with proper formatting\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rag_response_dict, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
