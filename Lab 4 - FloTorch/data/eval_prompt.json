{
    "eval_prompt": "Assume you are a human expert in grading predictions given by a model. You are given a question and a model prediction. Judge if the prediction matches the ground truth answer by following these steps:\n1: Take it as granted that the Ground Truth is always correct.\n2: If the Prediction indicates it is not sure about the answer or it shows that it has insufficient answer, \"score\" should be \"0\"; otherwise, go the next step.\n3: If the Prediction exactly matches the Ground Truth, \"score\" is 1.\n4: If the Prediction does not exactly match the Ground Truth, go through the following steps and likely give a score as 0.\n5: If the Ground Truth is a number, \"score\" is 1 if and only if the Prediction gives a number that almost exactly matches the ground truth.\n6: If the Prediction is self-contradictory, \"score\" must be 0.\n7: If the prediction is not answering the question, \"score\" must be 0.\n8: If the prediction is a concise and correct summary of the ground truth, \"score\" is 1.\n10: Otherwise, \"score\" is 0.\n\n### Output a JSON blob with an \"explanation\" field explaining your answer as short as possible and an \"score\" field with value 1 or 0.\n\nYou should make the judgment based on provided examples.\nExamples:\nQuestion: \"What are the three main sub-tasks in Knowledge Base Question Answering (KBQA) as identified in the paper?\"\nGround Truth: \"The three main sub-tasks in KBQA are topic entity detection, entity linking, and relation detection.\"\nPrediction: \"Sorry, I don't have sufficient information to provide an answer.\"\nOutput: {{\"score\": 0, \"explanation\": \"The prediction is not sure about the answer.\"}}\n\nQuestion: \"What is the role of BERT in the proposed KBQA framework?\"\nGround Truth: \"BERT is used to improve accuracy across all three sub-tasks by serving as a shared encoder in the multi-task learning framework.\"\nPrediction: \"BERT is used only for tokenization and does not contribute to the accuracy improvements in any of the sub-tasks within the multi-task learning framework.\"\nOutput: {{\"score\": 0, \"explanation\": \"The prediction doesn't match with ground truth\"}}\n\nQuestion: \"What is the significance of the PRACTIQ dataset in the context of real-world applications?\"\nGround Truth: \"The PRACTIQ dataset is significant for real-world applications as it introduces practical challenges faced in conversational text-to-SQL systems, such as handling ambiguous and unanswerable queries, thereby aiding in the development of more robust and user-friendly database query interfaces.\"\nPrediction: \"The PRACTIQ dataset is significant for real-world applications as it introduces practical challenges faced in conversational text-to-SQL systems, such as handling ambiguous and unanswerable queries, thereby aiding in the development of more robust and user-friendly database query interfaces.\"\nOutput: {{\"score\": 1, \"explanation\": \"The prediction matches the ground truth.\"}}\n\nQuestion: \"How does Amazon evaluate whether a model meets or exceeds a Critical Capability Threshold?\"\nGround Truth: \"Amazon conducts Critical Capability Evaluations using a combination of automated and human-in-the-loop strategies to assess if models demonstrate capabilities that meet or exceed the defined thresholds.\"\nPrediction: \"Amazon evaluates whether a model meets or exceeds a Critical Capability Threshold through a combination of automated and human-in-the-loop strategies. These evaluations include:\n\n1. **Automated Benchmarks**: Using state-of-the-art public benchmarks and internal proprietary test sets to compare candidate models.\n2. **Expert Red Teaming**: Engaging specialized firms and academics to test models for safety and security, particularly for risks requiring domain-specific expertise.\n3. **Uplift Studies**: Conducting studies to assess whether access to a model enhances human capability to perform tasks compared to other existing resources.\"\nOutput: {{\"score\": 1, \"explanation\": \"The prediction matches the ground truth.\"}}\n\nQuestion: {user_query}\\n Ground truth: {ground_truth}\\n Prediction: {prediction}\\n"
}